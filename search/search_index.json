{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Matillion Docs Welcome to the Matillion docs staging ground Welcome to the Matillion docs staging area. This statically generated website is created using mkdocs, and is hosted on GitHub Pages. Using this site, we can stage and preview pages, and share them with other Matillion colleagues without needing to worry about Document360's limited user count. Stylistically, the site is a work-in-progress right now. But it deploys markdown pages quickly and allows us to provide public previews! Navigation shortcuts Press the F key to activate the search bar. Press the ESC key to deactivate the search bar. Press the N key to navigate to the next page. Press the P key to navigate to the previous page. These shortcuts don't work when Caps Lock is ON. Some admonitions Note This site is a work-in-progress, and should be considered pre-alpha at best. Note You can collapse this note if you think it's not worth reading. Likely you already read it, though... Tip This is a tip. Bug Uh oh, there's a creepy crawly lurking.","title":"Home"},{"location":"#matillion-docs","text":"","title":"Matillion Docs"},{"location":"#welcome-to-the-matillion-docs-staging-ground","text":"Welcome to the Matillion docs staging area. This statically generated website is created using mkdocs, and is hosted on GitHub Pages. Using this site, we can stage and preview pages, and share them with other Matillion colleagues without needing to worry about Document360's limited user count. Stylistically, the site is a work-in-progress right now. But it deploys markdown pages quickly and allows us to provide public previews!","title":"Welcome to the Matillion docs staging ground"},{"location":"#navigation-shortcuts","text":"Press the F key to activate the search bar. Press the ESC key to deactivate the search bar. Press the N key to navigate to the next page. Press the P key to navigate to the previous page. These shortcuts don't work when Caps Lock is ON.","title":"Navigation shortcuts"},{"location":"#some-admonitions","text":"Note This site is a work-in-progress, and should be considered pre-alpha at best. Note You can collapse this note if you think it's not worth reading. Likely you already read it, though... Tip This is a tip. Bug Uh oh, there's a creepy crawly lurking.","title":"Some admonitions"},{"location":"data-loader/","text":"Overview Welcome to the Matillion Data Loader documentation site where we hope you can find answers to your questions and problems as well as discovering new things that will enhance your ETL journey. If you are looking for ticketed support for a Matillion ETL issue, please visit the Matillion Support Portal or see our Getting Support information. If you are still using Matillion Data Loader v1.x, please refer to the Matillion Data Loader v1 documentation site . Category Description Data Loader Overview An introduction to the functions and concepts required to use Matillion Data Loader. Administration Extended information aimed at users administering Matillion Data Loader for themselves or their organization. Deploying CDC Agents Detailed information on CDC agent installation which is a requirement of CDC pipelines. Sources Configuration information for all Batch and CDC pipeline sources. Destinations How to connect to supported Destinations for Batch and CDC pipelines. Troubleshooting Information on common issues in using CDC pipelines.","title":"What is Matillion Data Loader?"},{"location":"data-loader/#overview","text":"Welcome to the Matillion Data Loader documentation site where we hope you can find answers to your questions and problems as well as discovering new things that will enhance your ETL journey. If you are looking for ticketed support for a Matillion ETL issue, please visit the Matillion Support Portal or see our Getting Support information. If you are still using Matillion Data Loader v1.x, please refer to the Matillion Data Loader v1 documentation site . Category Description Data Loader Overview An introduction to the functions and concepts required to use Matillion Data Loader. Administration Extended information aimed at users administering Matillion Data Loader for themselves or their organization. Deploying CDC Agents Detailed information on CDC agent installation which is a requirement of CDC pipelines. Sources Configuration information for all Batch and CDC pipeline sources. Destinations How to connect to supported Destinations for Batch and CDC pipelines. Troubleshooting Information on common issues in using CDC pipelines.","title":"Overview"},{"location":"data-loader/connect-to-bigquery/","text":"Overview Connect to Google BigQuery via Matillion Data Loader and use it as your destination for batch-loading a pipeline. Prerequisites Read Set up Google BigQuery to make sure you are ready to connect to your Google BigQuery destination. Connect to Google BigQuery Click Add GCP credential to add a new set of credentials to connect to Google Cloud Platform (GCP). Property Description GCP credential label A name for the new GCP credentials. Access key ID A JSON file with a working GCP access key. Drag the JSON file into the box, or click anywhere in the box to upload. Click Test and save to confirm the credentials work, save them, and return to Connect to Google BigQuery . Property Description GCP credential A working Google Cloud Platform credential. Destination label A descriptive name for the Google BigQuery destination. Configure Google BigQuery Property Description Project A working Google Cloud project. To learn more, read Creating and managing projects . Dataset Datasets are top-level containers stored within projects for organizing and controlling access to your tables and views. To learn more, Introduction to datasets . Cloud storage area A working Cloud Storage bucket to store data inside. To learn more, read Create storage buckets . Table prefix An optional prefix to add to your destination table. When you are happy with your configuration, click Continue .","title":"Connect to Google BigQuery"},{"location":"data-loader/connect-to-bigquery/#overview","text":"Connect to Google BigQuery via Matillion Data Loader and use it as your destination for batch-loading a pipeline.","title":"Overview"},{"location":"data-loader/connect-to-bigquery/#prerequisites","text":"Read Set up Google BigQuery to make sure you are ready to connect to your Google BigQuery destination.","title":"Prerequisites"},{"location":"data-loader/connect-to-bigquery/#connect-to-google-bigquery","text":"Click Add GCP credential to add a new set of credentials to connect to Google Cloud Platform (GCP). Property Description GCP credential label A name for the new GCP credentials. Access key ID A JSON file with a working GCP access key. Drag the JSON file into the box, or click anywhere in the box to upload. Click Test and save to confirm the credentials work, save them, and return to Connect to Google BigQuery . Property Description GCP credential A working Google Cloud Platform credential. Destination label A descriptive name for the Google BigQuery destination.","title":"Connect to Google BigQuery"},{"location":"data-loader/connect-to-bigquery/#configure-google-bigquery","text":"Property Description Project A working Google Cloud project. To learn more, read Creating and managing projects . Dataset Datasets are top-level containers stored within projects for organizing and controlling access to your tables and views. To learn more, Introduction to datasets . Cloud storage area A working Cloud Storage bucket to store data inside. To learn more, read Create storage buckets . Table prefix An optional prefix to add to your destination table. When you are happy with your configuration, click Continue .","title":"Configure Google BigQuery"},{"location":"data-loader/connect-to-snowflake/","text":"Overview Connect to Snowflake via Matillion Data Loader and use it as your destination for batch-loading a pipeline. Prerequisites ACCOUNTADMIN role privileges in Snowflake, or privileges equivalent to the SECURITYADMIN and SYSADMIN roles. Connect to Snowflake Property Description Destination label A name for the destination. Account Your Snowflake account. Your account might contain the name, region, and cloud provider. Username Your Snowflake username. Password/Private key A managed entry representing your Snowflake login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of Snowflake connection options. Click Test and continue to test your settings and move forward. You can't continue if the test fails for any reason. Configure Snowflake Set up the Snowflake destination. Your login connection to Snowflake affects which roles, warehouses, databases, and schemas you can choose. Property Description Role An entity with privileges. Read Overview of Access Control to learn more. Warehouse A Snowflake warehouse. Read Overview of Warehouses to learn more. Target database A Snowflake database. Read Database, Schema, and Share DDL to learn more. Target schema A Snowflake schema to load the table into. Read Database, Schema, and Share DDL to learn more. Table prefix Add an optional prefix to your destination table. This field is empty by default.","title":"Connect to Snowflake"},{"location":"data-loader/connect-to-snowflake/#overview","text":"Connect to Snowflake via Matillion Data Loader and use it as your destination for batch-loading a pipeline.","title":"Overview"},{"location":"data-loader/connect-to-snowflake/#prerequisites","text":"ACCOUNTADMIN role privileges in Snowflake, or privileges equivalent to the SECURITYADMIN and SYSADMIN roles.","title":"Prerequisites"},{"location":"data-loader/connect-to-snowflake/#connect-to-snowflake","text":"Property Description Destination label A name for the destination. Account Your Snowflake account. Your account might contain the name, region, and cloud provider. Username Your Snowflake username. Password/Private key A managed entry representing your Snowflake login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of Snowflake connection options. Click Test and continue to test your settings and move forward. You can't continue if the test fails for any reason.","title":"Connect to Snowflake"},{"location":"data-loader/connect-to-snowflake/#configure-snowflake","text":"Set up the Snowflake destination. Your login connection to Snowflake affects which roles, warehouses, databases, and schemas you can choose. Property Description Role An entity with privileges. Read Overview of Access Control to learn more. Warehouse A Snowflake warehouse. Read Overview of Warehouses to learn more. Target database A Snowflake database. Read Database, Schema, and Share DDL to learn more. Target schema A Snowflake schema to load the table into. Read Database, Schema, and Share DDL to learn more. Table prefix Add an optional prefix to your destination table. This field is empty by default.","title":"Configure Snowflake"},{"location":"data-loader/getting-support/","text":"Overview At Matillion, we love hearing from our customers. Communication with customers often gives us the opportunity to help users get more from their Matillion Data Loader experience, so please get in touch. We welcome questions, issues, or even general feedback, because these forms of communication help us to make a better product for all Matillion Data Loader users. Contacting Us Whether you are getting started with Matillion Data Loader, need technical advice, or are attempting to troubleshoot issues with the client and related services, we are eager to help. Sometimes the in-client or documented Help isn't enough to solve the challenges our customers face, and the best way for users to get further assistance is to either raise a ticket at our support portal or email our support team directly. Visit our support portal at support.matillion.com , from here you can: Create an account. Raise a Support Ticket - every page on our support site has the Raise A Support Ticket button. Post a public question. Ensure your \"Case Category\" is set to \"Matillion Data Loader\" Please adhere to the below table when submitting tickets to Matillion Support: Severity Code Description Severity 1 Critical production outage Severity 2 High impact loss of service Severity 3 Medium, partial impact to service delivery Severity 4 Low impact, minor issue, or information request","title":"Getting support"},{"location":"data-loader/getting-support/#overview","text":"At Matillion, we love hearing from our customers. Communication with customers often gives us the opportunity to help users get more from their Matillion Data Loader experience, so please get in touch. We welcome questions, issues, or even general feedback, because these forms of communication help us to make a better product for all Matillion Data Loader users.","title":"Overview"},{"location":"data-loader/getting-support/#contacting-us","text":"Whether you are getting started with Matillion Data Loader, need technical advice, or are attempting to troubleshoot issues with the client and related services, we are eager to help. Sometimes the in-client or documented Help isn't enough to solve the challenges our customers face, and the best way for users to get further assistance is to either raise a ticket at our support portal or email our support team directly. Visit our support portal at support.matillion.com , from here you can: Create an account. Raise a Support Ticket - every page on our support site has the Raise A Support Ticket button. Post a public question. Ensure your \"Case Category\" is set to \"Matillion Data Loader\" Please adhere to the below table when submitting tickets to Matillion Support: Severity Code Description Severity 1 Critical production outage Severity 2 High impact loss of service Severity 3 Medium, partial impact to service delivery Severity 4 Low impact, minor issue, or information request","title":"Contacting Us"},{"location":"data-loader/mongodb/","text":"Overview MongoDB is a source-available cross-platform document-oriented database program. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard. Create pipeline In Matillion Data Loader, click Add pipeline . Choose MongoDB from the grid of data sources. Choose Batch Loading . Connect to MongoDB Configure the MongoDB database connection settings, specifying the following: Property Description Connection URL The server IP or DNS address of your MongoDB server endpoint. Database The name of your MongoDB database. Username A valid login username for your MongoDB database server. Password A managed entry representing your MongoDB database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Connection String URI Format for reference of MongoDB connection options. Note The advanced connection settings AuthSchema:SCRAM-SHA-1 and AuthDatabase:admin will be defined by default. If your source is an Atlas cluster, you must also specify the following advanced connection settings: UseSSL: true SlaveOK: true You may also need to set ReplicaSet depending on your configuration: ReplicaSet=<your replica set name> . For example: cluster0-shard-00-01-test.mongodb.net:27017 , cluster0-shard-00-02-test.mongodb.net:27017 Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. Flatten your objects Set the Flatten objects dropdown field to either Yes or No . The default is No . When Yes , Matillion Data Loader flattens and converts a nested data layer object into a new object with only one layer of key:value pairs. Choose tables Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward. Configure columns Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. Note Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table. Choose destination Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery. Set frequency Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"MongoDB"},{"location":"data-loader/mongodb/#overview","text":"MongoDB is a source-available cross-platform document-oriented database program. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard.","title":"Overview"},{"location":"data-loader/mongodb/#create-pipeline","text":"In Matillion Data Loader, click Add pipeline . Choose MongoDB from the grid of data sources. Choose Batch Loading .","title":"Create pipeline"},{"location":"data-loader/mongodb/#connect-to-mongodb","text":"Configure the MongoDB database connection settings, specifying the following: Property Description Connection URL The server IP or DNS address of your MongoDB server endpoint. Database The name of your MongoDB database. Username A valid login username for your MongoDB database server. Password A managed entry representing your MongoDB database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Connection String URI Format for reference of MongoDB connection options. Note The advanced connection settings AuthSchema:SCRAM-SHA-1 and AuthDatabase:admin will be defined by default. If your source is an Atlas cluster, you must also specify the following advanced connection settings: UseSSL: true SlaveOK: true You may also need to set ReplicaSet depending on your configuration: ReplicaSet=<your replica set name> . For example: cluster0-shard-00-01-test.mongodb.net:27017 , cluster0-shard-00-02-test.mongodb.net:27017 Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason.","title":"Connect to MongoDB"},{"location":"data-loader/mongodb/#flatten-your-objects","text":"Set the Flatten objects dropdown field to either Yes or No . The default is No . When Yes , Matillion Data Loader flattens and converts a nested data layer object into a new object with only one layer of key:value pairs.","title":"Flatten your objects"},{"location":"data-loader/mongodb/#choose-tables","text":"Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward.","title":"Choose tables"},{"location":"data-loader/mongodb/#configure-columns","text":"Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. Note Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table.","title":"Configure columns"},{"location":"data-loader/mongodb/#choose-destination","text":"Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery.","title":"Choose destination"},{"location":"data-loader/mongodb/#set-frequency","text":"Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"Set frequency"},{"location":"data-loader/mysql/","text":"Overview MySQL is an open-source relational database used to store custom data in-house. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard. Prerequisites Your MySQL database server must be running. Enable TCP/IP protocols with the TCP port set to 3306 . You must have access to your MySQL database host's IP address or domain. Make sure the MySQL database users has SELECT privileges. :::info You must have permission to access your MySQL database resources. ::: Create pipeline In Matillion Data Loader, click Add pipeline . Choose MySQL from the grid of data sources. Choose Batch Loading . Connect to MySQL Configure the MySQL database connection settings, specifying the following: Property Description Server address The URL required to connect to the MySQL database server. Port The required port number to connect to the MySQL database server. The default value is 3306 . Database name The name of the MySQL database you want to connect to. Username A valid login username for the MySQL database server. Password A managed entry representing your MySQL database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of MySQL connection options. Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. :::error If you encounter an error where zeroed dates are causing pipelines to fail with an error such as below: Value '0000-00-00' can not be represented as java.sql.Date or: Value '0000-00-00 00:00:00' can not be represented as java.sql.Timestamp You can solve this error by selecting the zeroDateTimeBehaviour parameter and assigning a value of convertToNull . ::: Choose tables Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward. Configure columns Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. :::info - Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. - Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. ::: Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table. Choose destination Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery. Set frequency Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"MySQL"},{"location":"data-loader/mysql/#overview","text":"MySQL is an open-source relational database used to store custom data in-house. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard.","title":"Overview"},{"location":"data-loader/mysql/#prerequisites","text":"Your MySQL database server must be running. Enable TCP/IP protocols with the TCP port set to 3306 . You must have access to your MySQL database host's IP address or domain. Make sure the MySQL database users has SELECT privileges. :::info You must have permission to access your MySQL database resources. :::","title":"Prerequisites"},{"location":"data-loader/mysql/#create-pipeline","text":"In Matillion Data Loader, click Add pipeline . Choose MySQL from the grid of data sources. Choose Batch Loading .","title":"Create pipeline"},{"location":"data-loader/mysql/#connect-to-mysql","text":"Configure the MySQL database connection settings, specifying the following: Property Description Server address The URL required to connect to the MySQL database server. Port The required port number to connect to the MySQL database server. The default value is 3306 . Database name The name of the MySQL database you want to connect to. Username A valid login username for the MySQL database server. Password A managed entry representing your MySQL database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of MySQL connection options. Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. :::error If you encounter an error where zeroed dates are causing pipelines to fail with an error such as below: Value '0000-00-00' can not be represented as java.sql.Date or: Value '0000-00-00 00:00:00' can not be represented as java.sql.Timestamp You can solve this error by selecting the zeroDateTimeBehaviour parameter and assigning a value of convertToNull . :::","title":"Connect to MySQL"},{"location":"data-loader/mysql/#choose-tables","text":"Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward.","title":"Choose tables"},{"location":"data-loader/mysql/#configure-columns","text":"Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. :::info - Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. - Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. ::: Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table.","title":"Configure columns"},{"location":"data-loader/mysql/#choose-destination","text":"Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery.","title":"Choose destination"},{"location":"data-loader/mysql/#set-frequency","text":"Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"Set frequency"},{"location":"data-loader/set-up-google-bigquery/","text":"Overview This page is a guide to setting up your Google Cloud Platform (GCP) account to use Google BigQuery as a destination within Matillion Data Loader. Prerequisites For security purposes, you may wish to set up a new Google Cloud Platform (GCP) account and project for use with Matillion Data Loader. Make sure you have access to a GCP project. You need permissions in the GCP project to create Identity Access Management (IAM) services accounts. Make sure you have access to a Google Cloud Platform project within Google BigQuery. You need to connect to an IAM service account that has access to BigQuery, Cloud Storage, and your project. You need permissions to create a Cloud Storage bucket and a BigQuery dataset. You need administrative permission to a Matillion ETL for BigQuery instance that has billing enabled. It's recommended to have billing enabled while performing data loading processes even if you're using the free trial option. Enable the GCP project API If you have sufficient access to enable the API, follow the steps below to enable the API in your own Google Cloud project: Navigate to APIs & Services in the Google Cloud console. Click Library \u2192 Private . If you don't see an API listed, you don't have access to enable it. Click the API you want to enable. Use the search field to filter out unwanted objects. Enable the following APIs: BigQuery API Cloud Key Management Service (KMS) API (optional) Cloud Pub Sub API (optional) Cloud Resource Manager API Cloud Storage API Google Sheets API (if you want to use a Google Sheets data source). In the page that displays information about the API, click Enable . Create a BigQuery dataset To create a dataset: Navigate to Google BigQuery . In the Explorer panel, select the more button (three vertical dots) on your project and click Create data set . Name the dataset in the Data set ID field. Set a dataset location . You can't change the dataset location once set. Select Enable table expiry if you wish to set a maximum age of a table in days. Click Create DATA SET . You can now choose this data set in the dropdown list within the project, and the Dataset ID can be used in Matillion ETL and Data Loader. Creating Service Accounts Matillion Data Loader requires you to set up an existing GCP project with Google BigQuery and GCP authentication. Authentication is achieved via service accounts in Google\u2014an account within your GCP project used by virtual machines (VMs). A service account has assigned roles and permissions, and access keys. To create a service account: Make sure you have the BigQuery API enabled. Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Click + Create service account Enter a service account name to display in the Google Cloud console. The Google Cloud console generates a service account ID based on this name. Edit the ID if necessary. You can't change the ID later. Optional: Enter a description of the service account. Click Continue to progress to part 2, Grant this service account access to the project (optional) . Optional: Choose one or more IAM roles to grant to the service account on the project. Click Continue to progress to part 3, Grant users access to this service account (optional) . Optional: In the Service account users role field, add members that can impersonate the service account. Optional: In the Service account admins role field, add members that can manage the service account. Click Done to create the service account. Locate your new service account in Service accounts . Add roles to a service account Navigate to IAM & Admin: IAM in the Google Cloud console. Click the current project, and then click Open . In the Permission tab, locate your service account (Principal) and click the edit button (pencil). Add a role. Click + ADD ANOTHER ROLE if applicable. Click SAVE . Ensure these roles are present: BigQuery Admin Project Viewer Storage Admin Link a service account to a Cloud Billing account Your GCP project must have billing enabled. You must define a Cloud Billing account outside a project and then you must apply the Cloud Billing account to a project at your discretion. Navigate to Billing in the Google Cloud console. Select a Cloud Billing account for your chosen project in the list. To learn more about changing a Cloud Billing account for projects, read _Enable, disable, or change billing for a project _ . To learn how to create a Cloud Billing account, _Create, modify, or close your self-serve Cloud Billing account _ . Access keys Service accounts use keys to access services. You need a key to configure your Matillion ETL and Data Loader instances. The access key format is below: { \"type\" : \"service_account\" , \"project_id\" : \"abcde\" , \"private_key_id\" : \"\" , \"private_key\" : \"\" , \"client_email\" : \"abcde@appspot.gserviceaccount.com\" , \"client_id\" : \"XXXXXXXXXXXXX\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/abcde%40appspot.gserviceaccount.com\" } To add an access key in GCP: Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Locate your service account and in the Actions column, click the more button (three vertical dots). Click Manage keys Click ADD KEY \u2192 Create new key \u2192 set the radio button to JSON and click CREATE . Your browser begins the downloading of the key to your computer. Cloud Storage bucket To create a new Cloud Storage bucket: Log in to the Google Cloud console at the Cloud Storage product. Click CREATE BUCKET Read Google's Create storage buckets to learn how to set up your new bucket.","title":"Set up Google BigQuery"},{"location":"data-loader/set-up-google-bigquery/#overview","text":"This page is a guide to setting up your Google Cloud Platform (GCP) account to use Google BigQuery as a destination within Matillion Data Loader.","title":"Overview"},{"location":"data-loader/set-up-google-bigquery/#prerequisites","text":"For security purposes, you may wish to set up a new Google Cloud Platform (GCP) account and project for use with Matillion Data Loader. Make sure you have access to a GCP project. You need permissions in the GCP project to create Identity Access Management (IAM) services accounts. Make sure you have access to a Google Cloud Platform project within Google BigQuery. You need to connect to an IAM service account that has access to BigQuery, Cloud Storage, and your project. You need permissions to create a Cloud Storage bucket and a BigQuery dataset. You need administrative permission to a Matillion ETL for BigQuery instance that has billing enabled. It's recommended to have billing enabled while performing data loading processes even if you're using the free trial option.","title":"Prerequisites"},{"location":"data-loader/set-up-google-bigquery/#enable-the-gcp-project-api","text":"If you have sufficient access to enable the API, follow the steps below to enable the API in your own Google Cloud project: Navigate to APIs & Services in the Google Cloud console. Click Library \u2192 Private . If you don't see an API listed, you don't have access to enable it. Click the API you want to enable. Use the search field to filter out unwanted objects. Enable the following APIs: BigQuery API Cloud Key Management Service (KMS) API (optional) Cloud Pub Sub API (optional) Cloud Resource Manager API Cloud Storage API Google Sheets API (if you want to use a Google Sheets data source). In the page that displays information about the API, click Enable .","title":"Enable the GCP project API"},{"location":"data-loader/set-up-google-bigquery/#create-a-bigquery-dataset","text":"To create a dataset: Navigate to Google BigQuery . In the Explorer panel, select the more button (three vertical dots) on your project and click Create data set . Name the dataset in the Data set ID field. Set a dataset location . You can't change the dataset location once set. Select Enable table expiry if you wish to set a maximum age of a table in days. Click Create DATA SET . You can now choose this data set in the dropdown list within the project, and the Dataset ID can be used in Matillion ETL and Data Loader.","title":"Create a BigQuery dataset"},{"location":"data-loader/set-up-google-bigquery/#creating-service-accounts","text":"Matillion Data Loader requires you to set up an existing GCP project with Google BigQuery and GCP authentication. Authentication is achieved via service accounts in Google\u2014an account within your GCP project used by virtual machines (VMs). A service account has assigned roles and permissions, and access keys. To create a service account: Make sure you have the BigQuery API enabled. Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Click + Create service account Enter a service account name to display in the Google Cloud console. The Google Cloud console generates a service account ID based on this name. Edit the ID if necessary. You can't change the ID later. Optional: Enter a description of the service account. Click Continue to progress to part 2, Grant this service account access to the project (optional) . Optional: Choose one or more IAM roles to grant to the service account on the project. Click Continue to progress to part 3, Grant users access to this service account (optional) . Optional: In the Service account users role field, add members that can impersonate the service account. Optional: In the Service account admins role field, add members that can manage the service account. Click Done to create the service account. Locate your new service account in Service accounts .","title":"Creating Service Accounts"},{"location":"data-loader/set-up-google-bigquery/#add-roles-to-a-service-account","text":"Navigate to IAM & Admin: IAM in the Google Cloud console. Click the current project, and then click Open . In the Permission tab, locate your service account (Principal) and click the edit button (pencil). Add a role. Click + ADD ANOTHER ROLE if applicable. Click SAVE . Ensure these roles are present: BigQuery Admin Project Viewer Storage Admin","title":"Add roles to a service account"},{"location":"data-loader/set-up-google-bigquery/#link-a-service-account-to-a-cloud-billing-account","text":"Your GCP project must have billing enabled. You must define a Cloud Billing account outside a project and then you must apply the Cloud Billing account to a project at your discretion. Navigate to Billing in the Google Cloud console. Select a Cloud Billing account for your chosen project in the list. To learn more about changing a Cloud Billing account for projects, read _Enable, disable, or change billing for a project _ . To learn how to create a Cloud Billing account, _Create, modify, or close your self-serve Cloud Billing account _ .","title":"Link a service account to a Cloud Billing account"},{"location":"data-loader/set-up-google-bigquery/#access-keys","text":"Service accounts use keys to access services. You need a key to configure your Matillion ETL and Data Loader instances. The access key format is below: { \"type\" : \"service_account\" , \"project_id\" : \"abcde\" , \"private_key_id\" : \"\" , \"private_key\" : \"\" , \"client_email\" : \"abcde@appspot.gserviceaccount.com\" , \"client_id\" : \"XXXXXXXXXXXXX\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/abcde%40appspot.gserviceaccount.com\" } To add an access key in GCP: Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Locate your service account and in the Actions column, click the more button (three vertical dots). Click Manage keys Click ADD KEY \u2192 Create new key \u2192 set the radio button to JSON and click CREATE . Your browser begins the downloading of the key to your computer.","title":"Access keys"},{"location":"data-loader/set-up-google-bigquery/#cloud-storage-bucket","text":"To create a new Cloud Storage bucket: Log in to the Google Cloud console at the Cloud Storage product. Click CREATE BUCKET Read Google's Create storage buckets to learn how to set up your new bucket.","title":"Cloud Storage bucket"},{"location":"etl/","text":"Some text.","title":"What is Matillion ETL?"},{"location":"etl/164-patch-notes/","text":"Overview Below, you can find release notes for Matillion ETL version 1.64.x Matillion ETL version 1.64.7 (major release) June 21, 2022 Tech notes Tech note - Shopify Query versioning Tech note - Splunk Query versioning New features and improvements All platforms Versioned the Shopify Query component (deprecated the component and replaced with a new version of Shopify Query component). Versioned the Splunk Query component (deprecated the component and replaced it with a new version of the Splunk Query component). Updated the Gmail Query component to include a new parameter, Data Schema . Users can set this to either IMAP or REST . Updated the Calculator component to include a grid variable dialog as an alternative to the expression editor. Updated Manage Schedules dialog. When you click into a schedule's Task Info , a separate Task Info dialog opens. Click OK to close this Task Info dialog and return to Manage Schedules . NULL values in Matillion ETL now display as italicized, capitalized, and with a grey text color (#757575). This change is to avoid confusion where a non-NULL value is Null. For example, if a person's surname is literally Null, for example, Jane Null or Tom Null. Updated Zero Copy Clone in the following ways: Added Run job in clone functionality. You can run any transformation or orchestration job using a cloned database. This lets you to test the job on data that mirrors that in your production environment, without affecting your production data. Improved error messages in the clone environment and database dialog. New Shared Job Git API endpoints are available including: Switch commit Delete branch Get state (logs) Create branch (on current commit) Get remote (URL) The Payload field in Manage Error Reporting is no longer editable when using a payload template that isn't [Custom] . These payloads are now read-only. Use Manage Webhook Payloads to edit payloads. Redshift The Avro file format is now available for the following components for Matillion ETL for Redshift: Create External Table External Table Output Rewrite External Table Deprecation All platforms The Google AdWords Query component is now disabled. This component was deprecated in version 1.62.7. Bug fixes All platforms Fixed an issue where maliciously altered exported jobs could be used for a Cross-Site Scripting (XSS) attack on import. Fixed an issue to make sure that Excel Query components running in iterators are consistently associated with their files. Updated Snowflake JDBC driver to latest version (3.13.18). This corrects a proxy regression, among other improvements. Fixed an issue where the API Extract component and Manage Extract Profiles wizard didn't pass a correct bearer token when Authentication Type = Bearer Token . Fixed an issue where additional parameters didn't pass in the request to get the Access Token when configuring an API OAuth, when Grant Type = Client Credentials. Redshift, Synapse Fixed an issue where the Create External Table component parameter Table Metadata didn't open successfully when the value of the data type in the dialog was NULL . Delta Lake on Databricks Fixed an issue where row counts were reported as 0 when Query components had executed despite the actual number of staged rows displaying correctly during job execution. Fixed an issue where certain components would not present metadata in the correct casing when using aliasing on columns. Driver updates Bing Search API driver updated. 20.0.7662.0 \u2192 21.0.8152.0 Dynamics 365 API driver updated. 19.0.7156.0 \u2192 21.0.8137.0 Elasticsearch API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 Email API driver updated. 19.0.7354.0 \u2192 21.0.8137.0 Excel API driver updated. 21.0.7829.0 \u2192 21.0.8137.0 HubSpot API driver updated. 21.0.7907.0 \u2192 21.0.8130.0 Google BigQuery API driver updated. 21.0.7930.0 \u2192 21.0.8152.0 LDAP API driver updated. 15.0.6121.0 \u2192 21.0.8137.0 Sage Intacct API driver updated. 19.0.7121.0 \u2192 21.0.8137.0 ServiceNow API driver updated. 21.0.7930.0 \u2192 21.0.8137.0 Shopify API driver updated. 19.0.7485.0 \u2192 21.0.8137.0 Splunk API driver updated. 19.0.7216.0 \u2192 21.0.8137.0 Square API driver updated. 21.0.7867.0 \u2192 21.0.8137.0 SurveyMonkey API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 YouTube Analytics API driver updated. 20.0.7628.0 \u2192 21.0.8137.0 Zoho CRM API driver updated. 20.0.7662.0 \u2192 21.0.8137.0","title":1.64},{"location":"etl/164-patch-notes/#overview","text":"Below, you can find release notes for Matillion ETL version 1.64.x","title":"Overview"},{"location":"etl/164-patch-notes/#matillion-etl-version-1647-major-release","text":"June 21, 2022","title":"Matillion ETL version 1.64.7 (major release)"},{"location":"etl/164-patch-notes/#tech-notes","text":"Tech note - Shopify Query versioning Tech note - Splunk Query versioning","title":"Tech notes"},{"location":"etl/164-patch-notes/#new-features-and-improvements","text":"","title":"New features and improvements"},{"location":"etl/164-patch-notes/#all-platforms","text":"Versioned the Shopify Query component (deprecated the component and replaced with a new version of Shopify Query component). Versioned the Splunk Query component (deprecated the component and replaced it with a new version of the Splunk Query component). Updated the Gmail Query component to include a new parameter, Data Schema . Users can set this to either IMAP or REST . Updated the Calculator component to include a grid variable dialog as an alternative to the expression editor. Updated Manage Schedules dialog. When you click into a schedule's Task Info , a separate Task Info dialog opens. Click OK to close this Task Info dialog and return to Manage Schedules . NULL values in Matillion ETL now display as italicized, capitalized, and with a grey text color (#757575). This change is to avoid confusion where a non-NULL value is Null. For example, if a person's surname is literally Null, for example, Jane Null or Tom Null. Updated Zero Copy Clone in the following ways: Added Run job in clone functionality. You can run any transformation or orchestration job using a cloned database. This lets you to test the job on data that mirrors that in your production environment, without affecting your production data. Improved error messages in the clone environment and database dialog. New Shared Job Git API endpoints are available including: Switch commit Delete branch Get state (logs) Create branch (on current commit) Get remote (URL) The Payload field in Manage Error Reporting is no longer editable when using a payload template that isn't [Custom] . These payloads are now read-only. Use Manage Webhook Payloads to edit payloads.","title":"All platforms"},{"location":"etl/164-patch-notes/#redshift","text":"The Avro file format is now available for the following components for Matillion ETL for Redshift: Create External Table External Table Output Rewrite External Table","title":"Redshift"},{"location":"etl/164-patch-notes/#deprecation","text":"","title":"Deprecation"},{"location":"etl/164-patch-notes/#all-platforms_1","text":"The Google AdWords Query component is now disabled. This component was deprecated in version 1.62.7.","title":"All platforms"},{"location":"etl/164-patch-notes/#bug-fixes","text":"","title":"Bug fixes"},{"location":"etl/164-patch-notes/#all-platforms_2","text":"Fixed an issue where maliciously altered exported jobs could be used for a Cross-Site Scripting (XSS) attack on import. Fixed an issue to make sure that Excel Query components running in iterators are consistently associated with their files. Updated Snowflake JDBC driver to latest version (3.13.18). This corrects a proxy regression, among other improvements. Fixed an issue where the API Extract component and Manage Extract Profiles wizard didn't pass a correct bearer token when Authentication Type = Bearer Token . Fixed an issue where additional parameters didn't pass in the request to get the Access Token when configuring an API OAuth, when Grant Type = Client Credentials.","title":"All platforms"},{"location":"etl/164-patch-notes/#redshift-synapse","text":"Fixed an issue where the Create External Table component parameter Table Metadata didn't open successfully when the value of the data type in the dialog was NULL .","title":"Redshift, Synapse"},{"location":"etl/164-patch-notes/#delta-lake-on-databricks","text":"Fixed an issue where row counts were reported as 0 when Query components had executed despite the actual number of staged rows displaying correctly during job execution. Fixed an issue where certain components would not present metadata in the correct casing when using aliasing on columns.","title":"Delta Lake on Databricks"},{"location":"etl/164-patch-notes/#driver-updates","text":"Bing Search API driver updated. 20.0.7662.0 \u2192 21.0.8152.0 Dynamics 365 API driver updated. 19.0.7156.0 \u2192 21.0.8137.0 Elasticsearch API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 Email API driver updated. 19.0.7354.0 \u2192 21.0.8137.0 Excel API driver updated. 21.0.7829.0 \u2192 21.0.8137.0 HubSpot API driver updated. 21.0.7907.0 \u2192 21.0.8130.0 Google BigQuery API driver updated. 21.0.7930.0 \u2192 21.0.8152.0 LDAP API driver updated. 15.0.6121.0 \u2192 21.0.8137.0 Sage Intacct API driver updated. 19.0.7121.0 \u2192 21.0.8137.0 ServiceNow API driver updated. 21.0.7930.0 \u2192 21.0.8137.0 Shopify API driver updated. 19.0.7485.0 \u2192 21.0.8137.0 Splunk API driver updated. 19.0.7216.0 \u2192 21.0.8137.0 Square API driver updated. 21.0.7867.0 \u2192 21.0.8137.0 SurveyMonkey API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 YouTube Analytics API driver updated. 20.0.7628.0 \u2192 21.0.8137.0 Zoho CRM API driver updated. 20.0.7662.0 \u2192 21.0.8137.0","title":"Driver updates"},{"location":"etl/google-query-authentication-guide/","text":"Some text.","title":"Google Query Authentication Guide"},{"location":"etl/google-sheets-query/","text":"Some text.","title":"Google Sheets Query"},{"location":"etl/switch-project/","text":"Some text.","title":"Switch Project"},{"location":"etl/test/","text":"Hello, this is just a test page. Open Visual Studio Code. Create a file with the extension .swift Run the following code: var gemCounter = 0 while gemCounter < 1 { moveForward () if ! isBlockedRight { turnRight () } if isBlocked && isBlockedRight { turnLeft () } if isBlocked && isBlockedRight && isBlockedLeft { turnLeft () } if isBlocked && isBlockedRight && ! isBlockedLeft { turnLeft () } if isOnGem { collectGem () gemCounter += 1 } } Now, what was the name of the variable above? Snowflake Redshift BigQuery Property Description Name A human-readable name for the component. Warehouse Select a Snowflake warehouse. Property Description Name A human-readable name for the component. Schema Select a Redshift schema. Property Description Name A human-readable name for the component. Dataset Select a BigQuery dataset.","title":"Test"},{"location":"matillion-hub/","text":"Overview The Matillion Hub is a single platform that offers a wide range of services for all registered Matillion Hub customers: Manage your subscription, edition and any billing configurations. View and manage your credit consumption across Matillion Data Loader (MDL) and Matillion ETL, and download your hourly usage by using an interactive dashboard. Add a new Matillion ETL instance, and view your existing ones. View and manage registered members associated with your organization, and manage account settings. Create pipelines using Batch and Change Data Capture (CDC) services. For more information, learn how to use the platform through our guides listed below. Getting Started You are required to register for a Matillion Hub account to gain full access to all services. The guides listed below provide instructions on how to register for an account and how to select the service you require, once you are registered and logged into the Matillion Hub: Registration Selecting a Service Administration Learn how to manage your Matillion Hub account from changing your account email address, editing user permissions, managing members in your organization, and resetting the password associated with your account: Change Email Address Edit User Permissions Manage Organization Members Resetting Password Services The Matillion Hub provides a wide-range of services. Use these guides listed below to learn more about them: Matillion Data Loader Matillion ETL: Matillion ETL Instance Creation Associating a Matillion ETL Instance Configuring a connection from Matillion ETL to the Matillion Hub Subscriptions Follow the steps in each of the guides listed below to set up and configure your chosen subscription, associated with your Matillion Hub account: AWS Subscription Marketplace Azure Subscription Marketplace After configuring your subscription, view its features, analyze and manage how you consume your credits on a regular basis, and modify your billing information: Editions Credit Consumption Dashboard Manage Subscription Payment Details & Invoices Next Step After the creation of a Matillion Hub account you will be able to do the following: - Create a Matillion ETL instance within the Hub. For more information, read Matillion ETL Instance Creation . - Add a Batch or CDC pipeline. For more information, read Matillion Data Loader Overview .","title":"What is Matillion Hub?"},{"location":"matillion-hub/#overview","text":"The Matillion Hub is a single platform that offers a wide range of services for all registered Matillion Hub customers: Manage your subscription, edition and any billing configurations. View and manage your credit consumption across Matillion Data Loader (MDL) and Matillion ETL, and download your hourly usage by using an interactive dashboard. Add a new Matillion ETL instance, and view your existing ones. View and manage registered members associated with your organization, and manage account settings. Create pipelines using Batch and Change Data Capture (CDC) services. For more information, learn how to use the platform through our guides listed below.","title":"Overview"},{"location":"matillion-hub/#getting-started","text":"You are required to register for a Matillion Hub account to gain full access to all services. The guides listed below provide instructions on how to register for an account and how to select the service you require, once you are registered and logged into the Matillion Hub: Registration Selecting a Service","title":"Getting Started"},{"location":"matillion-hub/#administration","text":"Learn how to manage your Matillion Hub account from changing your account email address, editing user permissions, managing members in your organization, and resetting the password associated with your account: Change Email Address Edit User Permissions Manage Organization Members Resetting Password","title":"Administration"},{"location":"matillion-hub/#services","text":"The Matillion Hub provides a wide-range of services. Use these guides listed below to learn more about them: Matillion Data Loader Matillion ETL: Matillion ETL Instance Creation Associating a Matillion ETL Instance Configuring a connection from Matillion ETL to the Matillion Hub","title":"Services"},{"location":"matillion-hub/#subscriptions","text":"Follow the steps in each of the guides listed below to set up and configure your chosen subscription, associated with your Matillion Hub account: AWS Subscription Marketplace Azure Subscription Marketplace After configuring your subscription, view its features, analyze and manage how you consume your credits on a regular basis, and modify your billing information: Editions Credit Consumption Dashboard Manage Subscription Payment Details & Invoices","title":"Subscriptions"},{"location":"matillion-hub/#next-step","text":"After the creation of a Matillion Hub account you will be able to do the following: - Create a Matillion ETL instance within the Hub. For more information, read Matillion ETL Instance Creation . - Add a Batch or CDC pipeline. For more information, read Matillion Data Loader Overview .","title":"Next Step"},{"location":"unlimited-scalability/","text":"Hello.","title":"Overview"},{"location":"unlimited-scalability/add-etl-agent-credentials-to-aws-secrets-manager/","text":"Overview This page is a guide to adding your ETL agent credentials to AWS Secrets Manager. Locate your ETL agent credentials Log in to Matillion Hub . Click Platform Navigation and choose Matillion Start . Choose Manage ETL Agents . Select an ETL agent. If you haven't created one yet, read Create an ETL agent . In Agent details , scroll down to Credentials . Click Reveal credentials . Add your credentials to AWS Secrets Manager Log in to the AWS Console . Once logged in, type \"Secrets Manager\" in the search bar and click Secrets Manager . Click Store a new secret . Choose the tile labelled Other type of secret . Add two key:value pairs: Key Value client_id The value of the client ID located via Matillion Start \u2192 Manage ETL Agents \u2192 select an ETL agent \u2192 Agent Details \u2192 Credentials \u2192 Reveal credentials . client_secret The value of the client secret located via Matillion Start \u2192 Manage ETL Agents \u2192 select an ETL agent \u2192 Agent details \u2192 Credentials \u2192 Reveal credentials . Click Next . Name the secret and provide a secret description. Click Next . Click Next again unless you wish to configure rotation settings. Review the secret and click Store . You'll return to Secrets . Refresh the page. Retrieve the ARN of your new secret While in the Secrets dashboard of AWS Secrets Manager, click the name of your new secret. In the Secret details container, copy the Secret ARN and save this value for later to reference it in the task definition.","title":"Add ETL agent credentials to AWS Secrets Manager"},{"location":"unlimited-scalability/add-etl-agent-credentials-to-aws-secrets-manager/#overview","text":"This page is a guide to adding your ETL agent credentials to AWS Secrets Manager.","title":"Overview"},{"location":"unlimited-scalability/add-etl-agent-credentials-to-aws-secrets-manager/#locate-your-etl-agent-credentials","text":"Log in to Matillion Hub . Click Platform Navigation and choose Matillion Start . Choose Manage ETL Agents . Select an ETL agent. If you haven't created one yet, read Create an ETL agent . In Agent details , scroll down to Credentials . Click Reveal credentials .","title":"Locate your ETL agent credentials"},{"location":"unlimited-scalability/add-etl-agent-credentials-to-aws-secrets-manager/#add-your-credentials-to-aws-secrets-manager","text":"Log in to the AWS Console . Once logged in, type \"Secrets Manager\" in the search bar and click Secrets Manager . Click Store a new secret . Choose the tile labelled Other type of secret . Add two key:value pairs: Key Value client_id The value of the client ID located via Matillion Start \u2192 Manage ETL Agents \u2192 select an ETL agent \u2192 Agent Details \u2192 Credentials \u2192 Reveal credentials . client_secret The value of the client secret located via Matillion Start \u2192 Manage ETL Agents \u2192 select an ETL agent \u2192 Agent details \u2192 Credentials \u2192 Reveal credentials . Click Next . Name the secret and provide a secret description. Click Next . Click Next again unless you wish to configure rotation settings. Review the secret and click Store . You'll return to Secrets . Refresh the page.","title":"Add your credentials to AWS Secrets Manager"},{"location":"unlimited-scalability/add-etl-agent-credentials-to-aws-secrets-manager/#retrieve-the-arn-of-your-new-secret","text":"While in the Secrets dashboard of AWS Secrets Manager, click the name of your new secret. In the Secret details container, copy the Secret ARN and save this value for later to reference it in the task definition.","title":"Retrieve the ARN of your new secret"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/","text":"Overview This page is a guide to manually installing an ETL agent into AWS Fargate. Prerequisites Each time you click Create in AWS, a coloured banner will appear at the top of the AWS UI. Blue indicates the creation of a resource is in-progress. Green indicates the creation of a resource has completed. Red indicates the creation of a resource has failed. Security groups Log in to the AWS Console . Once logged in, type security groups in the search bar. In the Features list, select Security Groups|EC2 feature . Choose Create security group . Complete the following fields: Property Description Security group name Add the following: matillion-etl-agent-<agentID> Your Agent ID is available at Agent environment variables in Matillion Hub \u2192 Matillion ETL Agents . Description Add the following description: Matillion ETL agent security group for agent ID <agentID> . Alternatively, add a description of your choice. VPC Select your VPC. Inbound rules Add any inbound rules. None are required by default. Outbound rules 0.0.0.0/0 Tags It's recommended to add a tag where Key = matillion-agent-id and Value = <agentID> . Click Create security group to confirm creation. On successful creation of the security group, you will be redirected to the new security group's dashboard. Please make a note of your Security group ID . Log groups In the AWS console, type CloudWatch in the search bar and navigate to AWS CloudWatch. In the left-hand menu, choose Logs \u2192 Log groups . Click Create log group . Complete the following fields: Property Description Log group name Add the following: /ecs/matillion-etl-agent/agentID . Retention setting Select a retention setting. It's recommended to choose 1 month (30 days) from the dropdown. KMS key ARN - optional Not required. Tags It's recommended to add a tag where Key = matillion-agent-id and Value = <agentID> . Click Create to confirm creation of the log group. On successful creation of the log group, you will be redirected to Log groups . Cluster In the AWS console, type Elastic Container Service in the search bar and choose that service. Click Get started or Clusters if this is your first time using this service. Click Create Cluster . Cluster configuration Property Description Cluster name Add the following: matillion-etl-agent-<agentID> VPC Select your VPC. Subnets Optional. Choose subnets where your tasks will run. Use Container Insights Toggle this to On . Tags It's recommended to add a tag where Key = matillion-agent-id and Value = <agentID> . Click Create to confirm creation of the cluster. On successful creation of the cluster, you will be redirected to All Clusters . Task definitions In the AWS console, type Elastic Container Service in the search bar and choose that service. In the left-hand menu, click Task definitions . Click Create new task definition . Complete the following fields: Task definition configuration Property Description Task definition family Add the following: matillion-etl-agent-<agentID> . This must be unique\u2014two task definitions can't share a name. Container - n Property Description Name matillion-etl-agent Image URI Copy and paste your Agent Image URI from Agent details . Port mappings Remove any existing entries. In Environment Variables under Add individually , add the following environment variables using the values in your Agent details page in Matillion Hub: Property Description AGENT_ID Your AGENT_ID value. ACCOUNT_ID Your ACCOUNT_ID value. OAUTH_SECRET_REF The Amazon resource number (ARN) of an AWS Secrets Manager secret that contains the client_id and client_secret located via Matillion Start \u2192 Manage ETL Agents \u2192 Agent Details \u2192 Credentials \u2192 Reveal credentials . To learn more about this process, read Add ETL agent credentials to AWS Secrets Manager . OAUTH_DOMAIN Your OAUTH_DOMAIN value. extensionLibraryLocation Your EXTENSION_LIBRARY_LOCATION value. agentGatewayURI Your AGENT_GATEWAY_URI value. Click Next . Configure environment, storage, monitoring, and tags Property Description App environment Choose AWS Fargate (serverless) CPU Choose 1 vCPU . Memory Choose 2 GB . Task role Choose ecsTaskExecutionRole . Scroll down to Monitoring and logging - optional . If the naming of components has been consistent so far, the awslogs-group value should equal the log group that was previously created (note the use of the / preceding the ID in the original Log group name). Otherwise, select the log group created earlier. Delete the aws-create-group entry by clicking Remove . Once again, add a tag where Key = matillion-agent-id and Value = <agentID> . Click Next . Review your settings and if you're happy, click Create . You will receive a blue banner at the top of the page explaining that AWS is currently creating the task definition. This banner will turn green upon completion. Service In your newly created task definition, click the Deploy dropdown button and then click Create service . In the Choose a cluster field, choose the cluster you created earlier. In Deployment configuration , complete the following fields: Property Description Service name Add the following: matillion-etl-agent-<agentID> . Desired tasks Use the default value of 1 . In Deployment options , set both Min running tasks and Max running tasks . The max must be greater than the min. In Networking , under Security group name , locate the security group you created earlier and select it. Remove any other security groups if they are not required. Ensure that Public IP is toggled to Enabled , unless you are using Network Address Translation. As in previous sections, add a tag where Key = matillion-agent-id and Value = <agentID> . Click Deploy . Once the deployment has finished, your ETL agent will be running.","title":"AWS Fargate manual agent setup"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#overview","text":"This page is a guide to manually installing an ETL agent into AWS Fargate.","title":"Overview"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#prerequisites","text":"Each time you click Create in AWS, a coloured banner will appear at the top of the AWS UI. Blue indicates the creation of a resource is in-progress. Green indicates the creation of a resource has completed. Red indicates the creation of a resource has failed.","title":"Prerequisites"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#security-groups","text":"Log in to the AWS Console . Once logged in, type security groups in the search bar. In the Features list, select Security Groups|EC2 feature . Choose Create security group . Complete the following fields: Property Description Security group name Add the following: matillion-etl-agent-<agentID> Your Agent ID is available at Agent environment variables in Matillion Hub \u2192 Matillion ETL Agents . Description Add the following description: Matillion ETL agent security group for agent ID <agentID> . Alternatively, add a description of your choice. VPC Select your VPC. Inbound rules Add any inbound rules. None are required by default. Outbound rules 0.0.0.0/0 Tags It's recommended to add a tag where Key = matillion-agent-id and Value = <agentID> . Click Create security group to confirm creation. On successful creation of the security group, you will be redirected to the new security group's dashboard. Please make a note of your Security group ID .","title":"Security groups"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#log-groups","text":"In the AWS console, type CloudWatch in the search bar and navigate to AWS CloudWatch. In the left-hand menu, choose Logs \u2192 Log groups . Click Create log group . Complete the following fields: Property Description Log group name Add the following: /ecs/matillion-etl-agent/agentID . Retention setting Select a retention setting. It's recommended to choose 1 month (30 days) from the dropdown. KMS key ARN - optional Not required. Tags It's recommended to add a tag where Key = matillion-agent-id and Value = <agentID> . Click Create to confirm creation of the log group. On successful creation of the log group, you will be redirected to Log groups .","title":"Log groups"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#cluster","text":"In the AWS console, type Elastic Container Service in the search bar and choose that service. Click Get started or Clusters if this is your first time using this service. Click Create Cluster .","title":"Cluster"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#cluster-configuration","text":"Property Description Cluster name Add the following: matillion-etl-agent-<agentID> VPC Select your VPC. Subnets Optional. Choose subnets where your tasks will run. Use Container Insights Toggle this to On . Tags It's recommended to add a tag where Key = matillion-agent-id and Value = <agentID> . Click Create to confirm creation of the cluster. On successful creation of the cluster, you will be redirected to All Clusters .","title":"Cluster configuration"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#task-definitions","text":"In the AWS console, type Elastic Container Service in the search bar and choose that service. In the left-hand menu, click Task definitions . Click Create new task definition . Complete the following fields: Task definition configuration Property Description Task definition family Add the following: matillion-etl-agent-<agentID> . This must be unique\u2014two task definitions can't share a name. Container - n Property Description Name matillion-etl-agent Image URI Copy and paste your Agent Image URI from Agent details . Port mappings Remove any existing entries. In Environment Variables under Add individually , add the following environment variables using the values in your Agent details page in Matillion Hub: Property Description AGENT_ID Your AGENT_ID value. ACCOUNT_ID Your ACCOUNT_ID value. OAUTH_SECRET_REF The Amazon resource number (ARN) of an AWS Secrets Manager secret that contains the client_id and client_secret located via Matillion Start \u2192 Manage ETL Agents \u2192 Agent Details \u2192 Credentials \u2192 Reveal credentials . To learn more about this process, read Add ETL agent credentials to AWS Secrets Manager . OAUTH_DOMAIN Your OAUTH_DOMAIN value. extensionLibraryLocation Your EXTENSION_LIBRARY_LOCATION value. agentGatewayURI Your AGENT_GATEWAY_URI value. Click Next .","title":"Task definitions"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#configure-environment-storage-monitoring-and-tags","text":"Property Description App environment Choose AWS Fargate (serverless) CPU Choose 1 vCPU . Memory Choose 2 GB . Task role Choose ecsTaskExecutionRole . Scroll down to Monitoring and logging - optional . If the naming of components has been consistent so far, the awslogs-group value should equal the log group that was previously created (note the use of the / preceding the ID in the original Log group name). Otherwise, select the log group created earlier. Delete the aws-create-group entry by clicking Remove . Once again, add a tag where Key = matillion-agent-id and Value = <agentID> . Click Next . Review your settings and if you're happy, click Create . You will receive a blue banner at the top of the page explaining that AWS is currently creating the task definition. This banner will turn green upon completion.","title":"Configure environment, storage, monitoring, and tags"},{"location":"unlimited-scalability/aws-fargate-manual-agent-setup/#service","text":"In your newly created task definition, click the Deploy dropdown button and then click Create service . In the Choose a cluster field, choose the cluster you created earlier. In Deployment configuration , complete the following fields: Property Description Service name Add the following: matillion-etl-agent-<agentID> . Desired tasks Use the default value of 1 . In Deployment options , set both Min running tasks and Max running tasks . The max must be greater than the min. In Networking , under Security group name , locate the security group you created earlier and select it. Remove any other security groups if they are not required. Ensure that Public IP is toggled to Enabled , unless you are using Network Address Translation. As in previous sections, add a tag where Key = matillion-agent-id and Value = <agentID> . Click Deploy . Once the deployment has finished, your ETL agent will be running.","title":"Service"},{"location":"unlimited-scalability/create-an-etl-agent/","text":"Overview Prerequisites You need a Matillion Hub account. To learn more, read Registration . Once you have signed up, log in to Matillion Hub . Create an ETL agent Navigate to the Select your service page. Get there by clicking the Platform Navigation button (the 9-dot square in the top-left of the UI) and then by clicking Matillion start . Click the Manage ETL Agents tile. If you haven't created an ETL agent yet, you'll land on Create your first ETL agent . Click Create an ETL agent . Property Description Agent name A unique name for your new ETL agent. Maximum 30 characters. Accepts both uppercase and lowercase A-z, 0-9, whitespace (not the first character), hyphens, underscores, and single quote characters. AWS region Select the AWS region to create your ETL agent within. New to regions? Read Regions and Availability Zones . Agent version This is the agent version track that will be deployed. The latest version in the displayed track will be used every time your service is restarted. When you are happy with the ETL agent configuration, click Create agent . Agent details The Agent details table lists provision metadata for your agent including the agent name, AWS region, and agent version set on the Create an ETL agent page. Also included are the agent type, the provisioning type, the cloud provider, and the deployment method. Agent image URI","title":"Create an ETL agent"},{"location":"unlimited-scalability/create-an-etl-agent/#overview","text":"","title":"Overview"},{"location":"unlimited-scalability/create-an-etl-agent/#prerequisites","text":"You need a Matillion Hub account. To learn more, read Registration . Once you have signed up, log in to Matillion Hub .","title":"Prerequisites"},{"location":"unlimited-scalability/create-an-etl-agent/#create-an-etl-agent","text":"Navigate to the Select your service page. Get there by clicking the Platform Navigation button (the 9-dot square in the top-left of the UI) and then by clicking Matillion start . Click the Manage ETL Agents tile. If you haven't created an ETL agent yet, you'll land on Create your first ETL agent . Click Create an ETL agent . Property Description Agent name A unique name for your new ETL agent. Maximum 30 characters. Accepts both uppercase and lowercase A-z, 0-9, whitespace (not the first character), hyphens, underscores, and single quote characters. AWS region Select the AWS region to create your ETL agent within. New to regions? Read Regions and Availability Zones . Agent version This is the agent version track that will be deployed. The latest version in the displayed track will be used every time your service is restarted. When you are happy with the ETL agent configuration, click Create agent .","title":"Create an ETL agent"},{"location":"unlimited-scalability/create-an-etl-agent/#agent-details","text":"The Agent details table lists provision metadata for your agent including the agent name, AWS region, and agent version set on the Create an ETL agent page. Also included are the agent type, the provisioning type, the cloud provider, and the deployment method.","title":"Agent details"},{"location":"unlimited-scalability/create-an-etl-agent/#agent-image-uri","text":"","title":"Agent image URI"}]}