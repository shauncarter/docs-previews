{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Matillion Docs Welcome to the Matillion docs staging ground Welcome to the Matillion docs staging area. This statically generated website is created using mkdocs, and is hosted on GitHub Pages. Using this site, we can stage and preview pages, and share them with other Matillion colleagues without needing to worry about Document360's limited user count. Stylistically, the site is a work-in-progress right now. But it deploys markdown pages quickly and allows us to provide public previews!","title":"Home"},{"location":"#matillion-docs","text":"","title":"Matillion Docs"},{"location":"#welcome-to-the-matillion-docs-staging-ground","text":"Welcome to the Matillion docs staging area. This statically generated website is created using mkdocs, and is hosted on GitHub Pages. Using this site, we can stage and preview pages, and share them with other Matillion colleagues without needing to worry about Document360's limited user count. Stylistically, the site is a work-in-progress right now. But it deploys markdown pages quickly and allows us to provide public previews!","title":"Welcome to the Matillion docs staging ground"},{"location":"164-patch-notes/","text":"Overview Below, you can find release notes for Matillion ETL version 1.64.x Matillion ETL version 1.64.7 (major release) June 21, 2022 Tech notes Tech note - Shopify Query versioning Tech note - Splunk Query versioning New features and improvements All platforms Versioned the Shopify Query component (deprecated the component and replaced with a new version of Shopify Query component). Versioned the Splunk Query component (deprecated the component and replaced it with a new version of the Splunk Query component). Updated the Gmail Query component to include a new parameter, Data Schema . Users can set this to either IMAP or REST . Updated the Calculator component to include a grid variable dialog as an alternative to the expression editor. Updated Manage Schedules dialog. When you click into a schedule's Task Info , a separate Task Info dialog opens. Click OK to close this Task Info dialog and return to Manage Schedules . NULL values in Matillion ETL now display as italicized, capitalized, and with a grey text color (#757575). This change is to avoid confusion where a non-NULL value is Null. For example, if a person's surname is literally Null, for example, Jane Null or Tom Null. Updated Zero Copy Clone in the following ways: Added Run job in clone functionality. You can run any transformation or orchestration job using a cloned database. This lets you to test the job on data that mirrors that in your production environment, without affecting your production data. Improved error messages in the clone environment and database dialog. New Shared Job Git API endpoints are available including: Switch commit Delete branch Get state (logs) Create branch (on current commit) Get remote (URL) The Payload field in Manage Error Reporting is no longer editable when using a payload template that isn't [Custom] . These payloads are now read-only. Use Manage Webhook Payloads to edit payloads. Redshift The Avro file format is now available for the following components for Matillion ETL for Redshift: Create External Table External Table Output Rewrite External Table Deprecation All platforms The Google AdWords Query component is now disabled. This component was deprecated in version 1.62.7. Bug fixes All platforms Fixed an issue where maliciously altered exported jobs could be used for a Cross-Site Scripting (XSS) attack on import. Fixed an issue to make sure that Excel Query components running in iterators are consistently associated with their files. Updated Snowflake JDBC driver to latest version (3.13.18). This corrects a proxy regression, among other improvements. Fixed an issue where the API Extract component and Manage Extract Profiles wizard didn't pass a correct bearer token when Authentication Type = Bearer Token . Fixed an issue where additional parameters didn't pass in the request to get the Access Token when configuring an API OAuth, when Grant Type = Client Credentials. Redshift, Synapse Fixed an issue where the Create External Table component parameter Table Metadata didn't open successfully when the value of the data type in the dialog was NULL . Delta Lake on Databricks Fixed an issue where row counts were reported as 0 when Query components had executed despite the actual number of staged rows displaying correctly during job execution. Fixed an issue where certain components would not present metadata in the correct casing when using aliasing on columns. Driver updates Bing Search API driver updated. 20.0.7662.0 \u2192 21.0.8152.0 Dynamics 365 API driver updated. 19.0.7156.0 \u2192 21.0.8137.0 Elasticsearch API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 Email API driver updated. 19.0.7354.0 \u2192 21.0.8137.0 Excel API driver updated. 21.0.7829.0 \u2192 21.0.8137.0 HubSpot API driver updated. 21.0.7907.0 \u2192 21.0.8130.0 Google BigQuery API driver updated. 21.0.7930.0 \u2192 21.0.8152.0 LDAP API driver updated. 15.0.6121.0 \u2192 21.0.8137.0 Sage Intacct API driver updated. 19.0.7121.0 \u2192 21.0.8137.0 ServiceNow API driver updated. 21.0.7930.0 \u2192 21.0.8137.0 Shopify API driver updated. 19.0.7485.0 \u2192 21.0.8137.0 Splunk API driver updated. 19.0.7216.0 \u2192 21.0.8137.0 Square API driver updated. 21.0.7867.0 \u2192 21.0.8137.0 SurveyMonkey API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 YouTube Analytics API driver updated. 20.0.7628.0 \u2192 21.0.8137.0 Zoho CRM API driver updated. 20.0.7662.0 \u2192 21.0.8137.0","title":"Matillion ETL 1.64"},{"location":"164-patch-notes/#overview","text":"Below, you can find release notes for Matillion ETL version 1.64.x","title":"Overview"},{"location":"164-patch-notes/#matillion-etl-version-1647-major-release","text":"June 21, 2022","title":"Matillion ETL version 1.64.7 (major release)"},{"location":"164-patch-notes/#tech-notes","text":"Tech note - Shopify Query versioning Tech note - Splunk Query versioning","title":"Tech notes"},{"location":"164-patch-notes/#new-features-and-improvements","text":"","title":"New features and improvements"},{"location":"164-patch-notes/#all-platforms","text":"Versioned the Shopify Query component (deprecated the component and replaced with a new version of Shopify Query component). Versioned the Splunk Query component (deprecated the component and replaced it with a new version of the Splunk Query component). Updated the Gmail Query component to include a new parameter, Data Schema . Users can set this to either IMAP or REST . Updated the Calculator component to include a grid variable dialog as an alternative to the expression editor. Updated Manage Schedules dialog. When you click into a schedule's Task Info , a separate Task Info dialog opens. Click OK to close this Task Info dialog and return to Manage Schedules . NULL values in Matillion ETL now display as italicized, capitalized, and with a grey text color (#757575). This change is to avoid confusion where a non-NULL value is Null. For example, if a person's surname is literally Null, for example, Jane Null or Tom Null. Updated Zero Copy Clone in the following ways: Added Run job in clone functionality. You can run any transformation or orchestration job using a cloned database. This lets you to test the job on data that mirrors that in your production environment, without affecting your production data. Improved error messages in the clone environment and database dialog. New Shared Job Git API endpoints are available including: Switch commit Delete branch Get state (logs) Create branch (on current commit) Get remote (URL) The Payload field in Manage Error Reporting is no longer editable when using a payload template that isn't [Custom] . These payloads are now read-only. Use Manage Webhook Payloads to edit payloads.","title":"All platforms"},{"location":"164-patch-notes/#redshift","text":"The Avro file format is now available for the following components for Matillion ETL for Redshift: Create External Table External Table Output Rewrite External Table","title":"Redshift"},{"location":"164-patch-notes/#deprecation","text":"","title":"Deprecation"},{"location":"164-patch-notes/#all-platforms_1","text":"The Google AdWords Query component is now disabled. This component was deprecated in version 1.62.7.","title":"All platforms"},{"location":"164-patch-notes/#bug-fixes","text":"","title":"Bug fixes"},{"location":"164-patch-notes/#all-platforms_2","text":"Fixed an issue where maliciously altered exported jobs could be used for a Cross-Site Scripting (XSS) attack on import. Fixed an issue to make sure that Excel Query components running in iterators are consistently associated with their files. Updated Snowflake JDBC driver to latest version (3.13.18). This corrects a proxy regression, among other improvements. Fixed an issue where the API Extract component and Manage Extract Profiles wizard didn't pass a correct bearer token when Authentication Type = Bearer Token . Fixed an issue where additional parameters didn't pass in the request to get the Access Token when configuring an API OAuth, when Grant Type = Client Credentials.","title":"All platforms"},{"location":"164-patch-notes/#redshift-synapse","text":"Fixed an issue where the Create External Table component parameter Table Metadata didn't open successfully when the value of the data type in the dialog was NULL .","title":"Redshift, Synapse"},{"location":"164-patch-notes/#delta-lake-on-databricks","text":"Fixed an issue where row counts were reported as 0 when Query components had executed despite the actual number of staged rows displaying correctly during job execution. Fixed an issue where certain components would not present metadata in the correct casing when using aliasing on columns.","title":"Delta Lake on Databricks"},{"location":"164-patch-notes/#driver-updates","text":"Bing Search API driver updated. 20.0.7662.0 \u2192 21.0.8152.0 Dynamics 365 API driver updated. 19.0.7156.0 \u2192 21.0.8137.0 Elasticsearch API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 Email API driver updated. 19.0.7354.0 \u2192 21.0.8137.0 Excel API driver updated. 21.0.7829.0 \u2192 21.0.8137.0 HubSpot API driver updated. 21.0.7907.0 \u2192 21.0.8130.0 Google BigQuery API driver updated. 21.0.7930.0 \u2192 21.0.8152.0 LDAP API driver updated. 15.0.6121.0 \u2192 21.0.8137.0 Sage Intacct API driver updated. 19.0.7121.0 \u2192 21.0.8137.0 ServiceNow API driver updated. 21.0.7930.0 \u2192 21.0.8137.0 Shopify API driver updated. 19.0.7485.0 \u2192 21.0.8137.0 Splunk API driver updated. 19.0.7216.0 \u2192 21.0.8137.0 Square API driver updated. 21.0.7867.0 \u2192 21.0.8137.0 SurveyMonkey API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 YouTube Analytics API driver updated. 20.0.7628.0 \u2192 21.0.8137.0 Zoho CRM API driver updated. 20.0.7662.0 \u2192 21.0.8137.0","title":"Driver updates"},{"location":"connect-to-bigquery/","text":"Overview Connect to Google BigQuery via Matillion Data Loader and use it as your destination for batch-loading a pipeline. Prerequisites Read Set up Google BigQuery to make sure you are ready to connect to your Google BigQuery destination. Connect to Google BigQuery Click Add GCP credential to add a new set of credentials to connect to Google Cloud Platform (GCP). Property Description GCP credential label A name for the new GCP credentials. Access key ID A JSON file with a working GCP access key. Drag the JSON file into the box, or click anywhere in the box to upload. Click Test and save to confirm the credentials work, save them, and return to Connect to Google BigQuery . Property Description GCP credential A working Google Cloud Platform credential. Destination label A descriptive name for the Google BigQuery destination. Configure Google BigQuery Property Description Project A working Google Cloud project. To learn more, read Creating and managing projects . Dataset Datasets are top-level containers stored within projects for organizing and controlling access to your tables and views. To learn more, Introduction to datasets . Cloud storage area A working Cloud Storage bucket to store data inside. To learn more, read Create storage buckets . Table prefix An optional prefix to add to your destination table. When you are happy with your configuration, click Continue .","title":"Connect to BigQuery"},{"location":"connect-to-bigquery/#overview","text":"Connect to Google BigQuery via Matillion Data Loader and use it as your destination for batch-loading a pipeline.","title":"Overview"},{"location":"connect-to-bigquery/#prerequisites","text":"Read Set up Google BigQuery to make sure you are ready to connect to your Google BigQuery destination.","title":"Prerequisites"},{"location":"connect-to-bigquery/#connect-to-google-bigquery","text":"Click Add GCP credential to add a new set of credentials to connect to Google Cloud Platform (GCP). Property Description GCP credential label A name for the new GCP credentials. Access key ID A JSON file with a working GCP access key. Drag the JSON file into the box, or click anywhere in the box to upload. Click Test and save to confirm the credentials work, save them, and return to Connect to Google BigQuery . Property Description GCP credential A working Google Cloud Platform credential. Destination label A descriptive name for the Google BigQuery destination.","title":"Connect to Google BigQuery"},{"location":"connect-to-bigquery/#configure-google-bigquery","text":"Property Description Project A working Google Cloud project. To learn more, read Creating and managing projects . Dataset Datasets are top-level containers stored within projects for organizing and controlling access to your tables and views. To learn more, Introduction to datasets . Cloud storage area A working Cloud Storage bucket to store data inside. To learn more, read Create storage buckets . Table prefix An optional prefix to add to your destination table. When you are happy with your configuration, click Continue .","title":"Configure Google BigQuery"},{"location":"connect-to-snowflake/","text":"Overview Connect to Snowflake via Matillion Data Loader and use it as your destination for batch-loading a pipeline. Prerequisites ACCOUNTADMIN role privileges in Snowflake, or privileges equivalent to the SECURITYADMIN and SYSADMIN roles. Connect to Snowflake Property Description Destination label A name for the destination. Account Your Snowflake account. Your account might contain the name, region, and cloud provider. Username Your Snowflake username. Password/Private key A managed entry representing your Snowflake login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of Snowflake connection options. Click Test and continue to test your settings and move forward. You can't continue if the test fails for any reason. Configure Snowflake Set up the Snowflake destination. Your login connection to Snowflake affects which roles, warehouses, databases, and schemas you can choose. Property Description Role An entity with privileges. Read Overview of Access Control to learn more. Warehouse A Snowflake warehouse. Read Overview of Warehouses to learn more. Target database A Snowflake database. Read Database, Schema, and Share DDL to learn more. Target schema A Snowflake schema to load the table into. Read Database, Schema, and Share DDL to learn more. Table prefix Add an optional prefix to your destination table. This field is empty by default.","title":"Snowflake"},{"location":"connect-to-snowflake/#overview","text":"Connect to Snowflake via Matillion Data Loader and use it as your destination for batch-loading a pipeline.","title":"Overview"},{"location":"connect-to-snowflake/#prerequisites","text":"ACCOUNTADMIN role privileges in Snowflake, or privileges equivalent to the SECURITYADMIN and SYSADMIN roles.","title":"Prerequisites"},{"location":"connect-to-snowflake/#connect-to-snowflake","text":"Property Description Destination label A name for the destination. Account Your Snowflake account. Your account might contain the name, region, and cloud provider. Username Your Snowflake username. Password/Private key A managed entry representing your Snowflake login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of Snowflake connection options. Click Test and continue to test your settings and move forward. You can't continue if the test fails for any reason.","title":"Connect to Snowflake"},{"location":"connect-to-snowflake/#configure-snowflake","text":"Set up the Snowflake destination. Your login connection to Snowflake affects which roles, warehouses, databases, and schemas you can choose. Property Description Role An entity with privileges. Read Overview of Access Control to learn more. Warehouse A Snowflake warehouse. Read Overview of Warehouses to learn more. Target database A Snowflake database. Read Database, Schema, and Share DDL to learn more. Target schema A Snowflake schema to load the table into. Read Database, Schema, and Share DDL to learn more. Table prefix Add an optional prefix to your destination table. This field is empty by default.","title":"Configure Snowflake"},{"location":"mysql/","text":"Overview MySQL is an open-source relational database used to store custom data in-house. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard. Prerequisites Your MySQL database server must be running. Enable TCP/IP protocols with the TCP port set to 3306 . You must have access to your MySQL database host's IP address or domain. Make sure the MySQL database users has SELECT privileges. :::info You must have permission to access your MySQL database resources. ::: Create pipeline In Matillion Data Loader, click Add pipeline . Choose MySQL from the grid of data sources. Choose Batch Loading . Connect to MySQL Configure the MySQL database connection settings, specifying the following: Property Description Server address The URL required to connect to the MySQL database server. Port The required port number to connect to the MySQL database server. The default value is 3306 . Database name The name of the MySQL database you want to connect to. Username A valid login username for the MySQL database server. Password A managed entry representing your MySQL database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of MySQL connection options. Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. :::error If you encounter an error where zeroed dates are causing pipelines to fail with an error such as below: Value '0000-00-00' can not be represented as java.sql.Date or: Value '0000-00-00 00:00:00' can not be represented as java.sql.Timestamp You can solve this error by selecting the zeroDateTimeBehaviour parameter and assigning a value of convertToNull . ::: Choose tables Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward. Configure columns Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. :::info - Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. - Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. ::: Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table. Choose destination Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery. Set frequency Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"MySQL"},{"location":"mysql/#overview","text":"MySQL is an open-source relational database used to store custom data in-house. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard.","title":"Overview"},{"location":"mysql/#prerequisites","text":"Your MySQL database server must be running. Enable TCP/IP protocols with the TCP port set to 3306 . You must have access to your MySQL database host's IP address or domain. Make sure the MySQL database users has SELECT privileges. :::info You must have permission to access your MySQL database resources. :::","title":"Prerequisites"},{"location":"mysql/#create-pipeline","text":"In Matillion Data Loader, click Add pipeline . Choose MySQL from the grid of data sources. Choose Batch Loading .","title":"Create pipeline"},{"location":"mysql/#connect-to-mysql","text":"Configure the MySQL database connection settings, specifying the following: Property Description Server address The URL required to connect to the MySQL database server. Port The required port number to connect to the MySQL database server. The default value is 3306 . Database name The name of the MySQL database you want to connect to. Username A valid login username for the MySQL database server. Password A managed entry representing your MySQL database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of MySQL connection options. Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. :::error If you encounter an error where zeroed dates are causing pipelines to fail with an error such as below: Value '0000-00-00' can not be represented as java.sql.Date or: Value '0000-00-00 00:00:00' can not be represented as java.sql.Timestamp You can solve this error by selecting the zeroDateTimeBehaviour parameter and assigning a value of convertToNull . :::","title":"Connect to MySQL"},{"location":"mysql/#choose-tables","text":"Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward.","title":"Choose tables"},{"location":"mysql/#configure-columns","text":"Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. :::info - Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. - Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. ::: Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table.","title":"Configure columns"},{"location":"mysql/#choose-destination","text":"Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery.","title":"Choose destination"},{"location":"mysql/#set-frequency","text":"Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"Set frequency"},{"location":"set-up-google-bigquery/","text":"Overview This page is a guide to setting up your Google Cloud Platform (GCP) account to use Google BigQuery as a destination within Matillion Data Loader. Prerequisites For security purposes, you may wish to set up a new Google Cloud Platform (GCP) account and project for use with Matillion Data Loader. Make sure you have access to a GCP project. You need permissions in the GCP project to create Identity Access Management (IAM) services accounts. Make sure you have access to a Google Cloud Platform project within Google BigQuery. You need to connect to an IAM service account that has access to BigQuery, Cloud Storage, and your project. You need permissions to create a Cloud Storage bucket and a BigQuery dataset. You need administrative permission to a Matillion ETL for BigQuery instance that has billing enabled. It's recommended to have billing enabled while performing data loading processes even if you're using the free trial option. Enable the GCP project API If you have sufficient access to enable the API, follow the steps below to enable the API in your own Google Cloud project: Navigate to APIs & Services in the Google Cloud console. Click Library \u2192 Private . If you don't see an API listed, you don't have access to enable it. Click the API you want to enable. Use the search field to filter out unwanted objects. Enable the following APIs: BigQuery API Cloud Key Management Service (KMS) API (optional) Cloud Pub Sub API (optional) Cloud Resource Manager API Cloud Storage API Google Sheets API (if you want to use a Google Sheets data source). In the page that displays information about the API, click Enable . Create a BigQuery dataset To create a dataset: Navigate to Google BigQuery . In the Explorer panel, select the more button (three vertical dots) on your project and click Create data set . Name the dataset in the Data set ID field. Set a dataset location . You can't change the dataset location once set. Select Enable table expiry if you wish to set a maximum age of a table in days. Click Create DATA SET . You can now choose this data set in the dropdown list within the project, and the Dataset ID can be used in Matillion ETL and Data Loader. Creating Service Accounts Matillion Data Loader requires you to set up an existing GCP project with Google BigQuery and GCP authentication. Authentication is achieved via service accounts in Google\u2014an account within your GCP project used by virtual machines (VMs). A service account has assigned roles and permissions, and access keys. To create a service account: Make sure you have the BigQuery API enabled. Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Click + Create service account Enter a service account name to display in the Google Cloud console. The Google Cloud console generates a service account ID based on this name. Edit the ID if necessary. You can't change the ID later. Optional: Enter a description of the service account. Click Continue to progress to part 2, Grant this service account access to the project (optional) . Optional: Choose one or more IAM roles to grant to the service account on the project. Click Continue to progress to part 3, Grant users access to this service account (optional) . Optional: In the Service account users role field, add members that can impersonate the service account. Optional: In the Service account admins role field, add members that can manage the service account. Click Done to create the service account. Locate your new service account in Service accounts . Add roles to a service account Navigate to IAM & Admin: IAM in the Google Cloud console. Click the current project, and then click Open . In the Permission tab, locate your service account (Principal) and click the edit button (pencil). Add a role. Click + ADD ANOTHER ROLE if applicable. Click SAVE . Ensure these roles are present: BigQuery Admin Project Viewer Storage Admin Link a service account to a Cloud Billing account Your GCP project must have billing enabled. You must define a Cloud Billing account outside a project and then you must apply the Cloud Billing account to a project at your discretion. Navigate to Billing in the Google Cloud console. Select a Cloud Billing account for your chosen project in the list. To learn more about changing a Cloud Billing account for projects, read _Enable, disable, or change billing for a project _ . To learn how to create a Cloud Billing account, _Create, modify, or close your self-serve Cloud Billing account _ . Access keys Service accounts use keys to access services. You need a key to configure your Matillion ETL and Data Loader instances. The access key format is below: { \"type\": \"service_account\", \"project_id\": \"abcde\", \"private_key_id\": \"\", \"private_key\": \"\", \"client_email\": \"abcde@appspot.gserviceaccount.com\", \"client_id\": \"XXXXXXXXXXXXX\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://accounts.google.com/o/oauth2/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abcde%40appspot.gserviceaccount.com\" } To add an access key in GCP: Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Locate your service account and in the Actions column, click the more button (three vertical dots). Click Manage keys Click ADD KEY \u2192 Create new key \u2192 set the radio button to JSON and click CREATE . Your browser begins the downloading of the key to your computer. Cloud Storage bucket To create a new Cloud Storage bucket: Log in to the Google Cloud console at the Cloud Storage product. Click CREATE BUCKET Read Google's Create storage buckets to learn how to set up your new bucket.","title":"Set up BigQuery"},{"location":"set-up-google-bigquery/#overview","text":"This page is a guide to setting up your Google Cloud Platform (GCP) account to use Google BigQuery as a destination within Matillion Data Loader.","title":"Overview"},{"location":"set-up-google-bigquery/#prerequisites","text":"For security purposes, you may wish to set up a new Google Cloud Platform (GCP) account and project for use with Matillion Data Loader. Make sure you have access to a GCP project. You need permissions in the GCP project to create Identity Access Management (IAM) services accounts. Make sure you have access to a Google Cloud Platform project within Google BigQuery. You need to connect to an IAM service account that has access to BigQuery, Cloud Storage, and your project. You need permissions to create a Cloud Storage bucket and a BigQuery dataset. You need administrative permission to a Matillion ETL for BigQuery instance that has billing enabled. It's recommended to have billing enabled while performing data loading processes even if you're using the free trial option.","title":"Prerequisites"},{"location":"set-up-google-bigquery/#enable-the-gcp-project-api","text":"If you have sufficient access to enable the API, follow the steps below to enable the API in your own Google Cloud project: Navigate to APIs & Services in the Google Cloud console. Click Library \u2192 Private . If you don't see an API listed, you don't have access to enable it. Click the API you want to enable. Use the search field to filter out unwanted objects. Enable the following APIs: BigQuery API Cloud Key Management Service (KMS) API (optional) Cloud Pub Sub API (optional) Cloud Resource Manager API Cloud Storage API Google Sheets API (if you want to use a Google Sheets data source). In the page that displays information about the API, click Enable .","title":"Enable the GCP project API"},{"location":"set-up-google-bigquery/#create-a-bigquery-dataset","text":"To create a dataset: Navigate to Google BigQuery . In the Explorer panel, select the more button (three vertical dots) on your project and click Create data set . Name the dataset in the Data set ID field. Set a dataset location . You can't change the dataset location once set. Select Enable table expiry if you wish to set a maximum age of a table in days. Click Create DATA SET . You can now choose this data set in the dropdown list within the project, and the Dataset ID can be used in Matillion ETL and Data Loader.","title":"Create a BigQuery dataset"},{"location":"set-up-google-bigquery/#creating-service-accounts","text":"Matillion Data Loader requires you to set up an existing GCP project with Google BigQuery and GCP authentication. Authentication is achieved via service accounts in Google\u2014an account within your GCP project used by virtual machines (VMs). A service account has assigned roles and permissions, and access keys. To create a service account: Make sure you have the BigQuery API enabled. Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Click + Create service account Enter a service account name to display in the Google Cloud console. The Google Cloud console generates a service account ID based on this name. Edit the ID if necessary. You can't change the ID later. Optional: Enter a description of the service account. Click Continue to progress to part 2, Grant this service account access to the project (optional) . Optional: Choose one or more IAM roles to grant to the service account on the project. Click Continue to progress to part 3, Grant users access to this service account (optional) . Optional: In the Service account users role field, add members that can impersonate the service account. Optional: In the Service account admins role field, add members that can manage the service account. Click Done to create the service account. Locate your new service account in Service accounts .","title":"Creating Service Accounts"},{"location":"set-up-google-bigquery/#add-roles-to-a-service-account","text":"Navigate to IAM & Admin: IAM in the Google Cloud console. Click the current project, and then click Open . In the Permission tab, locate your service account (Principal) and click the edit button (pencil). Add a role. Click + ADD ANOTHER ROLE if applicable. Click SAVE . Ensure these roles are present: BigQuery Admin Project Viewer Storage Admin","title":"Add roles to a service account"},{"location":"set-up-google-bigquery/#link-a-service-account-to-a-cloud-billing-account","text":"Your GCP project must have billing enabled. You must define a Cloud Billing account outside a project and then you must apply the Cloud Billing account to a project at your discretion. Navigate to Billing in the Google Cloud console. Select a Cloud Billing account for your chosen project in the list. To learn more about changing a Cloud Billing account for projects, read _Enable, disable, or change billing for a project _ . To learn how to create a Cloud Billing account, _Create, modify, or close your self-serve Cloud Billing account _ .","title":"Link a service account to a Cloud Billing account"},{"location":"set-up-google-bigquery/#access-keys","text":"Service accounts use keys to access services. You need a key to configure your Matillion ETL and Data Loader instances. The access key format is below: { \"type\": \"service_account\", \"project_id\": \"abcde\", \"private_key_id\": \"\", \"private_key\": \"\", \"client_email\": \"abcde@appspot.gserviceaccount.com\", \"client_id\": \"XXXXXXXXXXXXX\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://accounts.google.com/o/oauth2/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/abcde%40appspot.gserviceaccount.com\" } To add an access key in GCP: Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Locate your service account and in the Actions column, click the more button (three vertical dots). Click Manage keys Click ADD KEY \u2192 Create new key \u2192 set the radio button to JSON and click CREATE . Your browser begins the downloading of the key to your computer.","title":"Access keys"},{"location":"set-up-google-bigquery/#cloud-storage-bucket","text":"To create a new Cloud Storage bucket: Log in to the Google Cloud console at the Cloud Storage product. Click CREATE BUCKET Read Google's Create storage buckets to learn how to set up your new bucket.","title":"Cloud Storage bucket"},{"location":"test/","text":"Hello, this is just a test page.","title":"Test"}]}