{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Matillion Docs Welcome to the Matillion docs staging ground Welcome to the Matillion docs staging area. This statically generated website is created using mkdocs, and is hosted on GitHub Pages. Using this site, we can stage and preview pages, and share them with other Matillion colleagues without needing to worry about Document360's limited user count. Stylistically, the site is a work-in-progress right now. But it deploys markdown pages quickly and allows us to provide public previews! Navigation shortcuts Press the F key to activate the search bar. Press the ESC key to deactivate the search bar. Press the N key to navigate to the next page. Press the P key to navigate to the previous page. Some admonitions Note This site is a work-in-progress, and should be considered pre-alpha at best. Note You can collapse this note if you think it's not worth reading. Likely you already read it, though... Tip This is a tip. Bug Uh oh, there's a creepy crawly lurking.","title":"Home"},{"location":"#matillion-docs","text":"","title":"Matillion Docs"},{"location":"#welcome-to-the-matillion-docs-staging-ground","text":"Welcome to the Matillion docs staging area. This statically generated website is created using mkdocs, and is hosted on GitHub Pages. Using this site, we can stage and preview pages, and share them with other Matillion colleagues without needing to worry about Document360's limited user count. Stylistically, the site is a work-in-progress right now. But it deploys markdown pages quickly and allows us to provide public previews!","title":"Welcome to the Matillion docs staging ground"},{"location":"#navigation-shortcuts","text":"Press the F key to activate the search bar. Press the ESC key to deactivate the search bar. Press the N key to navigate to the next page. Press the P key to navigate to the previous page.","title":"Navigation shortcuts"},{"location":"#some-admonitions","text":"Note This site is a work-in-progress, and should be considered pre-alpha at best. Note You can collapse this note if you think it's not worth reading. Likely you already read it, though... Tip This is a tip. Bug Uh oh, there's a creepy crawly lurking.","title":"Some admonitions"},{"location":"164-patch-notes/","text":"Overview Below, you can find release notes for Matillion ETL version 1.64.x Matillion ETL version 1.64.7 (major release) June 21, 2022 Tech notes Tech note - Shopify Query versioning Tech note - Splunk Query versioning New features and improvements All platforms Versioned the Shopify Query component (deprecated the component and replaced with a new version of Shopify Query component). Versioned the Splunk Query component (deprecated the component and replaced it with a new version of the Splunk Query component). Updated the Gmail Query component to include a new parameter, Data Schema . Users can set this to either IMAP or REST . Updated the Calculator component to include a grid variable dialog as an alternative to the expression editor. Updated Manage Schedules dialog. When you click into a schedule's Task Info , a separate Task Info dialog opens. Click OK to close this Task Info dialog and return to Manage Schedules . NULL values in Matillion ETL now display as italicized, capitalized, and with a grey text color (#757575). This change is to avoid confusion where a non-NULL value is Null. For example, if a person's surname is literally Null, for example, Jane Null or Tom Null. Updated Zero Copy Clone in the following ways: Added Run job in clone functionality. You can run any transformation or orchestration job using a cloned database. This lets you to test the job on data that mirrors that in your production environment, without affecting your production data. Improved error messages in the clone environment and database dialog. New Shared Job Git API endpoints are available including: Switch commit Delete branch Get state (logs) Create branch (on current commit) Get remote (URL) The Payload field in Manage Error Reporting is no longer editable when using a payload template that isn't [Custom] . These payloads are now read-only. Use Manage Webhook Payloads to edit payloads. Redshift The Avro file format is now available for the following components for Matillion ETL for Redshift: Create External Table External Table Output Rewrite External Table Deprecation All platforms The Google AdWords Query component is now disabled. This component was deprecated in version 1.62.7. Bug fixes All platforms Fixed an issue where maliciously altered exported jobs could be used for a Cross-Site Scripting (XSS) attack on import. Fixed an issue to make sure that Excel Query components running in iterators are consistently associated with their files. Updated Snowflake JDBC driver to latest version (3.13.18). This corrects a proxy regression, among other improvements. Fixed an issue where the API Extract component and Manage Extract Profiles wizard didn't pass a correct bearer token when Authentication Type = Bearer Token . Fixed an issue where additional parameters didn't pass in the request to get the Access Token when configuring an API OAuth, when Grant Type = Client Credentials. Redshift, Synapse Fixed an issue where the Create External Table component parameter Table Metadata didn't open successfully when the value of the data type in the dialog was NULL . Delta Lake on Databricks Fixed an issue where row counts were reported as 0 when Query components had executed despite the actual number of staged rows displaying correctly during job execution. Fixed an issue where certain components would not present metadata in the correct casing when using aliasing on columns. Driver updates Bing Search API driver updated. 20.0.7662.0 \u2192 21.0.8152.0 Dynamics 365 API driver updated. 19.0.7156.0 \u2192 21.0.8137.0 Elasticsearch API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 Email API driver updated. 19.0.7354.0 \u2192 21.0.8137.0 Excel API driver updated. 21.0.7829.0 \u2192 21.0.8137.0 HubSpot API driver updated. 21.0.7907.0 \u2192 21.0.8130.0 Google BigQuery API driver updated. 21.0.7930.0 \u2192 21.0.8152.0 LDAP API driver updated. 15.0.6121.0 \u2192 21.0.8137.0 Sage Intacct API driver updated. 19.0.7121.0 \u2192 21.0.8137.0 ServiceNow API driver updated. 21.0.7930.0 \u2192 21.0.8137.0 Shopify API driver updated. 19.0.7485.0 \u2192 21.0.8137.0 Splunk API driver updated. 19.0.7216.0 \u2192 21.0.8137.0 Square API driver updated. 21.0.7867.0 \u2192 21.0.8137.0 SurveyMonkey API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 YouTube Analytics API driver updated. 20.0.7628.0 \u2192 21.0.8137.0 Zoho CRM API driver updated. 20.0.7662.0 \u2192 21.0.8137.0","title":"164 patch notes"},{"location":"164-patch-notes/#overview","text":"Below, you can find release notes for Matillion ETL version 1.64.x","title":"Overview"},{"location":"164-patch-notes/#matillion-etl-version-1647-major-release","text":"June 21, 2022","title":"Matillion ETL version 1.64.7 (major release)"},{"location":"164-patch-notes/#tech-notes","text":"Tech note - Shopify Query versioning Tech note - Splunk Query versioning","title":"Tech notes"},{"location":"164-patch-notes/#new-features-and-improvements","text":"","title":"New features and improvements"},{"location":"164-patch-notes/#all-platforms","text":"Versioned the Shopify Query component (deprecated the component and replaced with a new version of Shopify Query component). Versioned the Splunk Query component (deprecated the component and replaced it with a new version of the Splunk Query component). Updated the Gmail Query component to include a new parameter, Data Schema . Users can set this to either IMAP or REST . Updated the Calculator component to include a grid variable dialog as an alternative to the expression editor. Updated Manage Schedules dialog. When you click into a schedule's Task Info , a separate Task Info dialog opens. Click OK to close this Task Info dialog and return to Manage Schedules . NULL values in Matillion ETL now display as italicized, capitalized, and with a grey text color (#757575). This change is to avoid confusion where a non-NULL value is Null. For example, if a person's surname is literally Null, for example, Jane Null or Tom Null. Updated Zero Copy Clone in the following ways: Added Run job in clone functionality. You can run any transformation or orchestration job using a cloned database. This lets you to test the job on data that mirrors that in your production environment, without affecting your production data. Improved error messages in the clone environment and database dialog. New Shared Job Git API endpoints are available including: Switch commit Delete branch Get state (logs) Create branch (on current commit) Get remote (URL) The Payload field in Manage Error Reporting is no longer editable when using a payload template that isn't [Custom] . These payloads are now read-only. Use Manage Webhook Payloads to edit payloads.","title":"All platforms"},{"location":"164-patch-notes/#redshift","text":"The Avro file format is now available for the following components for Matillion ETL for Redshift: Create External Table External Table Output Rewrite External Table","title":"Redshift"},{"location":"164-patch-notes/#deprecation","text":"","title":"Deprecation"},{"location":"164-patch-notes/#all-platforms_1","text":"The Google AdWords Query component is now disabled. This component was deprecated in version 1.62.7.","title":"All platforms"},{"location":"164-patch-notes/#bug-fixes","text":"","title":"Bug fixes"},{"location":"164-patch-notes/#all-platforms_2","text":"Fixed an issue where maliciously altered exported jobs could be used for a Cross-Site Scripting (XSS) attack on import. Fixed an issue to make sure that Excel Query components running in iterators are consistently associated with their files. Updated Snowflake JDBC driver to latest version (3.13.18). This corrects a proxy regression, among other improvements. Fixed an issue where the API Extract component and Manage Extract Profiles wizard didn't pass a correct bearer token when Authentication Type = Bearer Token . Fixed an issue where additional parameters didn't pass in the request to get the Access Token when configuring an API OAuth, when Grant Type = Client Credentials.","title":"All platforms"},{"location":"164-patch-notes/#redshift-synapse","text":"Fixed an issue where the Create External Table component parameter Table Metadata didn't open successfully when the value of the data type in the dialog was NULL .","title":"Redshift, Synapse"},{"location":"164-patch-notes/#delta-lake-on-databricks","text":"Fixed an issue where row counts were reported as 0 when Query components had executed despite the actual number of staged rows displaying correctly during job execution. Fixed an issue where certain components would not present metadata in the correct casing when using aliasing on columns.","title":"Delta Lake on Databricks"},{"location":"164-patch-notes/#driver-updates","text":"Bing Search API driver updated. 20.0.7662.0 \u2192 21.0.8152.0 Dynamics 365 API driver updated. 19.0.7156.0 \u2192 21.0.8137.0 Elasticsearch API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 Email API driver updated. 19.0.7354.0 \u2192 21.0.8137.0 Excel API driver updated. 21.0.7829.0 \u2192 21.0.8137.0 HubSpot API driver updated. 21.0.7907.0 \u2192 21.0.8130.0 Google BigQuery API driver updated. 21.0.7930.0 \u2192 21.0.8152.0 LDAP API driver updated. 15.0.6121.0 \u2192 21.0.8137.0 Sage Intacct API driver updated. 19.0.7121.0 \u2192 21.0.8137.0 ServiceNow API driver updated. 21.0.7930.0 \u2192 21.0.8137.0 Shopify API driver updated. 19.0.7485.0 \u2192 21.0.8137.0 Splunk API driver updated. 19.0.7216.0 \u2192 21.0.8137.0 Square API driver updated. 21.0.7867.0 \u2192 21.0.8137.0 SurveyMonkey API driver updated. 20.0.7662.0 \u2192 21.0.8137.0 YouTube Analytics API driver updated. 20.0.7628.0 \u2192 21.0.8137.0 Zoho CRM API driver updated. 20.0.7662.0 \u2192 21.0.8137.0","title":"Driver updates"},{"location":"connect-to-bigquery/","text":"Overview Connect to Google BigQuery via Matillion Data Loader and use it as your destination for batch-loading a pipeline. Prerequisites Read Set up Google BigQuery to make sure you are ready to connect to your Google BigQuery destination. Connect to Google BigQuery Click Add GCP credential to add a new set of credentials to connect to Google Cloud Platform (GCP). Property Description GCP credential label A name for the new GCP credentials. Access key ID A JSON file with a working GCP access key. Drag the JSON file into the box, or click anywhere in the box to upload. Click Test and save to confirm the credentials work, save them, and return to Connect to Google BigQuery . Property Description GCP credential A working Google Cloud Platform credential. Destination label A descriptive name for the Google BigQuery destination. Configure Google BigQuery Property Description Project A working Google Cloud project. To learn more, read Creating and managing projects . Dataset Datasets are top-level containers stored within projects for organizing and controlling access to your tables and views. To learn more, Introduction to datasets . Cloud storage area A working Cloud Storage bucket to store data inside. To learn more, read Create storage buckets . Table prefix An optional prefix to add to your destination table. When you are happy with your configuration, click Continue .","title":"Connect to bigquery"},{"location":"connect-to-bigquery/#overview","text":"Connect to Google BigQuery via Matillion Data Loader and use it as your destination for batch-loading a pipeline.","title":"Overview"},{"location":"connect-to-bigquery/#prerequisites","text":"Read Set up Google BigQuery to make sure you are ready to connect to your Google BigQuery destination.","title":"Prerequisites"},{"location":"connect-to-bigquery/#connect-to-google-bigquery","text":"Click Add GCP credential to add a new set of credentials to connect to Google Cloud Platform (GCP). Property Description GCP credential label A name for the new GCP credentials. Access key ID A JSON file with a working GCP access key. Drag the JSON file into the box, or click anywhere in the box to upload. Click Test and save to confirm the credentials work, save them, and return to Connect to Google BigQuery . Property Description GCP credential A working Google Cloud Platform credential. Destination label A descriptive name for the Google BigQuery destination.","title":"Connect to Google BigQuery"},{"location":"connect-to-bigquery/#configure-google-bigquery","text":"Property Description Project A working Google Cloud project. To learn more, read Creating and managing projects . Dataset Datasets are top-level containers stored within projects for organizing and controlling access to your tables and views. To learn more, Introduction to datasets . Cloud storage area A working Cloud Storage bucket to store data inside. To learn more, read Create storage buckets . Table prefix An optional prefix to add to your destination table. When you are happy with your configuration, click Continue .","title":"Configure Google BigQuery"},{"location":"connect-to-snowflake/","text":"Overview Connect to Snowflake via Matillion Data Loader and use it as your destination for batch-loading a pipeline. Prerequisites ACCOUNTADMIN role privileges in Snowflake, or privileges equivalent to the SECURITYADMIN and SYSADMIN roles. Connect to Snowflake Property Description Destination label A name for the destination. Account Your Snowflake account. Your account might contain the name, region, and cloud provider. Username Your Snowflake username. Password/Private key A managed entry representing your Snowflake login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of Snowflake connection options. Click Test and continue to test your settings and move forward. You can't continue if the test fails for any reason. Configure Snowflake Set up the Snowflake destination. Your login connection to Snowflake affects which roles, warehouses, databases, and schemas you can choose. Property Description Role An entity with privileges. Read Overview of Access Control to learn more. Warehouse A Snowflake warehouse. Read Overview of Warehouses to learn more. Target database A Snowflake database. Read Database, Schema, and Share DDL to learn more. Target schema A Snowflake schema to load the table into. Read Database, Schema, and Share DDL to learn more. Table prefix Add an optional prefix to your destination table. This field is empty by default.","title":"Connect to snowflake"},{"location":"connect-to-snowflake/#overview","text":"Connect to Snowflake via Matillion Data Loader and use it as your destination for batch-loading a pipeline.","title":"Overview"},{"location":"connect-to-snowflake/#prerequisites","text":"ACCOUNTADMIN role privileges in Snowflake, or privileges equivalent to the SECURITYADMIN and SYSADMIN roles.","title":"Prerequisites"},{"location":"connect-to-snowflake/#connect-to-snowflake","text":"Property Description Destination label A name for the destination. Account Your Snowflake account. Your account might contain the name, region, and cloud provider. Username Your Snowflake username. Password/Private key A managed entry representing your Snowflake login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of Snowflake connection options. Click Test and continue to test your settings and move forward. You can't continue if the test fails for any reason.","title":"Connect to Snowflake"},{"location":"connect-to-snowflake/#configure-snowflake","text":"Set up the Snowflake destination. Your login connection to Snowflake affects which roles, warehouses, databases, and schemas you can choose. Property Description Role An entity with privileges. Read Overview of Access Control to learn more. Warehouse A Snowflake warehouse. Read Overview of Warehouses to learn more. Target database A Snowflake database. Read Database, Schema, and Share DDL to learn more. Target schema A Snowflake schema to load the table into. Read Database, Schema, and Share DDL to learn more. Table prefix Add an optional prefix to your destination table. This field is empty by default.","title":"Configure Snowflake"},{"location":"manage-sqs-configuration/","text":"Overview When using Matillion ETL as part of a larger process, the best way to initiate an orchestration job is to use Amazon Simple Queue Service {:target=\"_blank\"} (SQS). Other applications can read messages posted to an SQS queue and perform further processing. You can put messages onto an SQS queue using Python or the SQS Message component . Setting up To configure SQS for Matillion ETL, first create up to three SQS queues as described in Getting started with Amazon SQS {:target=\"_blank\"}. In Matillion ETL, click Project \u2192 Manage SQS Configuration . Complete the following fields in the Manage SQS Configuration dialog. Property Description Enable SQS This turns on or off the sub-system that listens to SQS queues. Note: this is global, not for a particular project. Credentials Choose the credentials that you will be using to talk to SQS queues. An IAM user or Role will need the AmazonSQSFullAccess policy if you want to have a success or failure queue. Region The region where the queue exists in SQS. Listen Queue The name of the queue to listen for messages. These messages are in a set format as shown below . Enable Success Select this if you wish to place a message on an SQS queue when your orchestration job has completed. Success Queue The name of the success queue on which to place success messages. Compress When selected, the body of the message on the queue will be gzipped. Use this to avoid hitting SQS limits {:target=\"_blank\"}. Enable Failure Select this if you wish to place a message on an SQS queue when your orchestration job has failed. Failure Queue The name of the failure queue on which to place failure messages. Compress When selected, the body of the message on the queue will be gzipped. Use this to avoid hitting SQS limits {:target=\"_blank\"}. Note The queues for success and failure can't be FIFO queues, as there is no property set for the message ID and message de-duplication ID. Click Test to verify that the queues exist and can be read. Returned messages The messages that are returned on the success or fail queues are in the following format: { \"type\" : \"QUEUE_ORCHESTRATION\" , \"groupName\" : \"<the group name that was executed>\" , \"projectName\" : \"<the project name that was executed>\" , \"versionName\" : \"<the version name that was executed>\" , \"environmentName\" : \"<the version name that was executed>\" , \"state\" : \"SUCCESS|FAILED\" , \"enqueuedTime\" : <Time message placed o n queue i n u n ix epoc f orma t > , \"startTime\" : <Time orches trat io n job bega n i n u n ix epoc f orma t > , \"endTime\" : <Time orches trat io n job e n ded i n u n ix epoc f orma t > , \"message\" : <co nta i ns error messages where applicable> , \"originator ID\" : \"q_Queue\" , \"tasks\" :[ <This is a lis t o f tas ks execu te d i n t he orches trat io n > { \"type\" : \"VALIDATE_ORCHESTRATION\" , \"jobName\" : \"SimpleQueueJob\" , \"componentName\" : \"Start 0\" , \"orchestrationJobName\" : \"SimpleQueueJob\" , \"orchestrationComponentName\" : \"Start 0\" , \"state\" : \"SUCCESS\" , \"rowCount\" : 0 , \"startTime\" : 1443526622364 , \"endTime\" : 1443526622364 , \"message\" : \"\" }, { \"type\" : \"VALIDATE_ORCHESTRATION\" , \"jobName\" : \"SimpleQueueJob\" , \"componentName\" : \"End Success 0\" , \"orchestrationJobName\" : \"SimpleQueueJob\" , \"orchestrationComponentName\" : \"End Success 0\" , \"state\" : \"SUCCESS\" , \"rowCount\" : 0 , \"startTime\" : 1443526622365 , \"endTime\" : 1443526622369 , \"message\" : \"\" }, { \"type\" : \"EXECUTE_ORCHESTRATION\" , \"jobName\" : \"SimpleQueueJob\" , \"componentName\" : \"End Success 0\" , \"orchestrationJobName\" : \"SimpleQueueJob\" , \"orchestrationComponentName\" : \"End Success 0\" , \"state\" : \"SUCCESS\" , \"rowCount\" : 0 , \"startTime\" : 1443526622369 , \"endTime\" : 1443526622369 , \"message\" : \"\" } ], \"rowCount\" : 0 } Send messages using Python To put messages onto an SQS queue, you can adapt this Python snippet, or use any other AWS API at your disposal to achieve the same result. import boto.sqs import json from boto.sqs.message import RawMessage #connect to your region conn = boto . sqs . connect_to_region ( \"eu-west-1\" ) #create the queue to post messages queue = conn . create_queue ( \"QUEUE-NAME\" ) #set the queue to allow raw messages data queue . set_message_class ( RawMessage ) #prepare the message p_message = { \"created\" : \"-\" , \"group\" : \"myGroup\" , \"project\" : \"myproject\" , \"version\" : \"default\" , \"environment\" : \"myEnv\" , \"job\" : \"myjob\" } #prepare the message on the queue message = queue . new_message ( json . dumps ( p_message )) #write the message to the queue queue . write ( message ) Send messages using the SQS Message component You can use the SQS Message {:target=\"_blank\"} component in an orchestration job to post a message to an SQS queue. You specify the message properties (such as Queue Name and Region ) when you configure the component as part of a job. The message to be passed to the queue is written in plain text in the component's Message property. The message text can contain variables {:target=\"_blank\"} to be resolved at runtime, in the same way that other Matillion ETL components can use variables. The receiving queue must already exist so that you can select it when configuring the component. The following example shows the SQS Message component configured to send a message with the text \"This is a plain test message being sent to SQS\": Message format Messages are in a set format as follows. { \"group\":\"<Exactly matches the project group where your job resides>\", \"project\":\"<Exactly matches the project name where your job resides>\", \"version\":\"<Exactly matches the version Name>\", \"environment\":\"<Exactly matches the environment Name>\", \"job\":\"<Exactly matches the orchestration Job Name>\", \"variables\": { \"<Name 1>\": \"<Value 1>\", \"<Name 2>\": \"<Value 3>\" } \"gridVariables\" : { \"<GridVar Name>\": [[\"<R1C1 Value>\", \"<R1C2 Value>\"], [\"<R2C1 Value>\", \"<R2C2 Value>\"]] } } The variables and gridVariables fields are optional. Variables are passed to the orchestration job. Matching variable names must be declared in the project with default values set for each environment. If a variable is passed which isn't defined in the project, an error is logged in Project \u2192 Task History . Read the articles on Variables {:target=\"_blank\"} for more information.","title":"Manage sqs configuration"},{"location":"manage-sqs-configuration/#overview","text":"When using Matillion ETL as part of a larger process, the best way to initiate an orchestration job is to use Amazon Simple Queue Service {:target=\"_blank\"} (SQS). Other applications can read messages posted to an SQS queue and perform further processing. You can put messages onto an SQS queue using Python or the SQS Message component .","title":"Overview"},{"location":"manage-sqs-configuration/#setting-up","text":"To configure SQS for Matillion ETL, first create up to three SQS queues as described in Getting started with Amazon SQS {:target=\"_blank\"}. In Matillion ETL, click Project \u2192 Manage SQS Configuration . Complete the following fields in the Manage SQS Configuration dialog. Property Description Enable SQS This turns on or off the sub-system that listens to SQS queues. Note: this is global, not for a particular project. Credentials Choose the credentials that you will be using to talk to SQS queues. An IAM user or Role will need the AmazonSQSFullAccess policy if you want to have a success or failure queue. Region The region where the queue exists in SQS. Listen Queue The name of the queue to listen for messages. These messages are in a set format as shown below . Enable Success Select this if you wish to place a message on an SQS queue when your orchestration job has completed. Success Queue The name of the success queue on which to place success messages. Compress When selected, the body of the message on the queue will be gzipped. Use this to avoid hitting SQS limits {:target=\"_blank\"}. Enable Failure Select this if you wish to place a message on an SQS queue when your orchestration job has failed. Failure Queue The name of the failure queue on which to place failure messages. Compress When selected, the body of the message on the queue will be gzipped. Use this to avoid hitting SQS limits {:target=\"_blank\"}.","title":"Setting up"},{"location":"manage-sqs-configuration/#note","text":"The queues for success and failure can't be FIFO queues, as there is no property set for the message ID and message de-duplication ID. Click Test to verify that the queues exist and can be read.","title":"Note"},{"location":"manage-sqs-configuration/#returned-messages","text":"The messages that are returned on the success or fail queues are in the following format: { \"type\" : \"QUEUE_ORCHESTRATION\" , \"groupName\" : \"<the group name that was executed>\" , \"projectName\" : \"<the project name that was executed>\" , \"versionName\" : \"<the version name that was executed>\" , \"environmentName\" : \"<the version name that was executed>\" , \"state\" : \"SUCCESS|FAILED\" , \"enqueuedTime\" : <Time message placed o n queue i n u n ix epoc f orma t > , \"startTime\" : <Time orches trat io n job bega n i n u n ix epoc f orma t > , \"endTime\" : <Time orches trat io n job e n ded i n u n ix epoc f orma t > , \"message\" : <co nta i ns error messages where applicable> , \"originator ID\" : \"q_Queue\" , \"tasks\" :[ <This is a lis t o f tas ks execu te d i n t he orches trat io n > { \"type\" : \"VALIDATE_ORCHESTRATION\" , \"jobName\" : \"SimpleQueueJob\" , \"componentName\" : \"Start 0\" , \"orchestrationJobName\" : \"SimpleQueueJob\" , \"orchestrationComponentName\" : \"Start 0\" , \"state\" : \"SUCCESS\" , \"rowCount\" : 0 , \"startTime\" : 1443526622364 , \"endTime\" : 1443526622364 , \"message\" : \"\" }, { \"type\" : \"VALIDATE_ORCHESTRATION\" , \"jobName\" : \"SimpleQueueJob\" , \"componentName\" : \"End Success 0\" , \"orchestrationJobName\" : \"SimpleQueueJob\" , \"orchestrationComponentName\" : \"End Success 0\" , \"state\" : \"SUCCESS\" , \"rowCount\" : 0 , \"startTime\" : 1443526622365 , \"endTime\" : 1443526622369 , \"message\" : \"\" }, { \"type\" : \"EXECUTE_ORCHESTRATION\" , \"jobName\" : \"SimpleQueueJob\" , \"componentName\" : \"End Success 0\" , \"orchestrationJobName\" : \"SimpleQueueJob\" , \"orchestrationComponentName\" : \"End Success 0\" , \"state\" : \"SUCCESS\" , \"rowCount\" : 0 , \"startTime\" : 1443526622369 , \"endTime\" : 1443526622369 , \"message\" : \"\" } ], \"rowCount\" : 0 }","title":"Returned messages"},{"location":"manage-sqs-configuration/#send-messages-using-python","text":"To put messages onto an SQS queue, you can adapt this Python snippet, or use any other AWS API at your disposal to achieve the same result. import boto.sqs import json from boto.sqs.message import RawMessage #connect to your region conn = boto . sqs . connect_to_region ( \"eu-west-1\" ) #create the queue to post messages queue = conn . create_queue ( \"QUEUE-NAME\" ) #set the queue to allow raw messages data queue . set_message_class ( RawMessage ) #prepare the message p_message = { \"created\" : \"-\" , \"group\" : \"myGroup\" , \"project\" : \"myproject\" , \"version\" : \"default\" , \"environment\" : \"myEnv\" , \"job\" : \"myjob\" } #prepare the message on the queue message = queue . new_message ( json . dumps ( p_message )) #write the message to the queue queue . write ( message )","title":"Send messages using Python"},{"location":"manage-sqs-configuration/#send-messages-using-the-sqs-message-component","text":"You can use the SQS Message {:target=\"_blank\"} component in an orchestration job to post a message to an SQS queue. You specify the message properties (such as Queue Name and Region ) when you configure the component as part of a job. The message to be passed to the queue is written in plain text in the component's Message property. The message text can contain variables {:target=\"_blank\"} to be resolved at runtime, in the same way that other Matillion ETL components can use variables. The receiving queue must already exist so that you can select it when configuring the component. The following example shows the SQS Message component configured to send a message with the text \"This is a plain test message being sent to SQS\":","title":"Send messages using the SQS Message component"},{"location":"manage-sqs-configuration/#message-format","text":"Messages are in a set format as follows. { \"group\":\"<Exactly matches the project group where your job resides>\", \"project\":\"<Exactly matches the project name where your job resides>\", \"version\":\"<Exactly matches the version Name>\", \"environment\":\"<Exactly matches the environment Name>\", \"job\":\"<Exactly matches the orchestration Job Name>\", \"variables\": { \"<Name 1>\": \"<Value 1>\", \"<Name 2>\": \"<Value 3>\" } \"gridVariables\" : { \"<GridVar Name>\": [[\"<R1C1 Value>\", \"<R1C2 Value>\"], [\"<R2C1 Value>\", \"<R2C2 Value>\"]] } } The variables and gridVariables fields are optional. Variables are passed to the orchestration job. Matching variable names must be declared in the project with default values set for each environment. If a variable is passed which isn't defined in the project, an error is logged in Project \u2192 Task History . Read the articles on Variables {:target=\"_blank\"} for more information.","title":"Message format"},{"location":"matillion-data-loader/","text":"Overview Welcome to the Matillion Data Loader documentation site where we hope you can find answers to your questions and problems as well as discovering new things that will enhance your ETL journey. If you are looking for ticketed support for a Matillion ETL issue, please visit the Matillion Support Portal {:target=\"_blank\"} or see our Getting Support {:target=\"_blank\"} information. If you are still using Matillion Data Loader v1.x, please refer to the Matillion Data Loader v1 documentation site {:target=\"_blank\"}. Category Description Data Loader Overview {:target=\"_blank\"} An introduction to the functions and concepts required to use Matillion Data Loader. Administration {:target=\"_blank\"} Extended information aimed at users administering Matillion Data Loader for themselves or their organization. Deploying CDC Agents {:target=\"_blank\"} Detailed information on CDC agent installation which is a requirement of CDC pipelines. Sources {:target=\"_blank\"} Configuration information for all Batch and CDC pipeline sources. Destinations {:target=\"_blank\"} How to connect to supported Destinations for Batch and CDC pipelines. Troubleshooting {:target=\"_blank\"} Information on common issues in using CDC pipelines.","title":"Matillion data loader"},{"location":"matillion-data-loader/#overview","text":"Welcome to the Matillion Data Loader documentation site where we hope you can find answers to your questions and problems as well as discovering new things that will enhance your ETL journey. If you are looking for ticketed support for a Matillion ETL issue, please visit the Matillion Support Portal {:target=\"_blank\"} or see our Getting Support {:target=\"_blank\"} information. If you are still using Matillion Data Loader v1.x, please refer to the Matillion Data Loader v1 documentation site {:target=\"_blank\"}. Category Description Data Loader Overview {:target=\"_blank\"} An introduction to the functions and concepts required to use Matillion Data Loader. Administration {:target=\"_blank\"} Extended information aimed at users administering Matillion Data Loader for themselves or their organization. Deploying CDC Agents {:target=\"_blank\"} Detailed information on CDC agent installation which is a requirement of CDC pipelines. Sources {:target=\"_blank\"} Configuration information for all Batch and CDC pipeline sources. Destinations {:target=\"_blank\"} How to connect to supported Destinations for Batch and CDC pipelines. Troubleshooting {:target=\"_blank\"} Information on common issues in using CDC pipelines.","title":"Overview"},{"location":"mysql/","text":"Overview MySQL is an open-source relational database used to store custom data in-house. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard. Prerequisites Your MySQL database server must be running. Enable TCP/IP protocols with the TCP port set to 3306 . You must have access to your MySQL database host's IP address or domain. Make sure the MySQL database users has SELECT privileges. :::info You must have permission to access your MySQL database resources. ::: Create pipeline In Matillion Data Loader, click Add pipeline . Choose MySQL from the grid of data sources. Choose Batch Loading . Connect to MySQL Configure the MySQL database connection settings, specifying the following: Property Description Server address The URL required to connect to the MySQL database server. Port The required port number to connect to the MySQL database server. The default value is 3306 . Database name The name of the MySQL database you want to connect to. Username A valid login username for the MySQL database server. Password A managed entry representing your MySQL database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of MySQL connection options. Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. :::error If you encounter an error where zeroed dates are causing pipelines to fail with an error such as below: Value '0000-00-00' can not be represented as java.sql.Date or: Value '0000-00-00 00:00:00' can not be represented as java.sql.Timestamp You can solve this error by selecting the zeroDateTimeBehaviour parameter and assigning a value of convertToNull . ::: Choose tables Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward. Configure columns Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. :::info - Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. - Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. ::: Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table. Choose destination Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery. Set frequency Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"Mysql"},{"location":"mysql/#overview","text":"MySQL is an open-source relational database used to store custom data in-house. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard.","title":"Overview"},{"location":"mysql/#prerequisites","text":"Your MySQL database server must be running. Enable TCP/IP protocols with the TCP port set to 3306 . You must have access to your MySQL database host's IP address or domain. Make sure the MySQL database users has SELECT privileges. :::info You must have permission to access your MySQL database resources. :::","title":"Prerequisites"},{"location":"mysql/#create-pipeline","text":"In Matillion Data Loader, click Add pipeline . Choose MySQL from the grid of data sources. Choose Batch Loading .","title":"Create pipeline"},{"location":"mysql/#connect-to-mysql","text":"Configure the MySQL database connection settings, specifying the following: Property Description Server address The URL required to connect to the MySQL database server. Port The required port number to connect to the MySQL database server. The default value is 3306 . Database name The name of the MySQL database you want to connect to. Username A valid login username for the MySQL database server. Password A managed entry representing your MySQL database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Configuration Properties for reference of MySQL connection options. Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. :::error If you encounter an error where zeroed dates are causing pipelines to fail with an error such as below: Value '0000-00-00' can not be represented as java.sql.Date or: Value '0000-00-00 00:00:00' can not be represented as java.sql.Timestamp You can solve this error by selecting the zeroDateTimeBehaviour parameter and assigning a value of convertToNull . :::","title":"Connect to MySQL"},{"location":"mysql/#choose-tables","text":"Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward.","title":"Choose tables"},{"location":"mysql/#configure-columns","text":"Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. :::info - Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. - Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. ::: Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table.","title":"Configure columns"},{"location":"mysql/#choose-destination","text":"Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery.","title":"Choose destination"},{"location":"mysql/#set-frequency","text":"Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"Set frequency"},{"location":"resources/","text":"","title":"Resources"},{"location":"set-up-google-bigquery/","text":"Overview This page is a guide to setting up your Google Cloud Platform (GCP) account to use Google BigQuery as a destination within Matillion Data Loader. Prerequisites For security purposes, you may wish to set up a new Google Cloud Platform (GCP) account and project for use with Matillion Data Loader. Make sure you have access to a GCP project. You need permissions in the GCP project to create Identity Access Management (IAM) services accounts. Make sure you have access to a Google Cloud Platform project within Google BigQuery. You need to connect to an IAM service account that has access to BigQuery, Cloud Storage, and your project. You need permissions to create a Cloud Storage bucket and a BigQuery dataset. You need administrative permission to a Matillion ETL for BigQuery instance that has billing enabled. It's recommended to have billing enabled while performing data loading processes even if you're using the free trial option. Enable the GCP project API If you have sufficient access to enable the API, follow the steps below to enable the API in your own Google Cloud project: Navigate to APIs & Services in the Google Cloud console. Click Library \u2192 Private . If you don't see an API listed, you don't have access to enable it. Click the API you want to enable. Use the search field to filter out unwanted objects. Enable the following APIs: BigQuery API Cloud Key Management Service (KMS) API (optional) Cloud Pub Sub API (optional) Cloud Resource Manager API Cloud Storage API Google Sheets API (if you want to use a Google Sheets data source). In the page that displays information about the API, click Enable . Create a BigQuery dataset To create a dataset: Navigate to Google BigQuery . In the Explorer panel, select the more button (three vertical dots) on your project and click Create data set . Name the dataset in the Data set ID field. Set a dataset location . You can't change the dataset location once set. Select Enable table expiry if you wish to set a maximum age of a table in days. Click Create DATA SET . You can now choose this data set in the dropdown list within the project, and the Dataset ID can be used in Matillion ETL and Data Loader. Creating Service Accounts Matillion Data Loader requires you to set up an existing GCP project with Google BigQuery and GCP authentication. Authentication is achieved via service accounts in Google\u2014an account within your GCP project used by virtual machines (VMs). A service account has assigned roles and permissions, and access keys. To create a service account: Make sure you have the BigQuery API enabled. Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Click + Create service account Enter a service account name to display in the Google Cloud console. The Google Cloud console generates a service account ID based on this name. Edit the ID if necessary. You can't change the ID later. Optional: Enter a description of the service account. Click Continue to progress to part 2, Grant this service account access to the project (optional) . Optional: Choose one or more IAM roles to grant to the service account on the project. Click Continue to progress to part 3, Grant users access to this service account (optional) . Optional: In the Service account users role field, add members that can impersonate the service account. Optional: In the Service account admins role field, add members that can manage the service account. Click Done to create the service account. Locate your new service account in Service accounts . Add roles to a service account Navigate to IAM & Admin: IAM in the Google Cloud console. Click the current project, and then click Open . In the Permission tab, locate your service account (Principal) and click the edit button (pencil). Add a role. Click + ADD ANOTHER ROLE if applicable. Click SAVE . Ensure these roles are present: BigQuery Admin Project Viewer Storage Admin Link a service account to a Cloud Billing account Your GCP project must have billing enabled. You must define a Cloud Billing account outside a project and then you must apply the Cloud Billing account to a project at your discretion. Navigate to Billing in the Google Cloud console. Select a Cloud Billing account for your chosen project in the list. To learn more about changing a Cloud Billing account for projects, read _Enable, disable, or change billing for a project _ . To learn how to create a Cloud Billing account, _Create, modify, or close your self-serve Cloud Billing account _ . Access keys Service accounts use keys to access services. You need a key to configure your Matillion ETL and Data Loader instances. The access key format is below: { \"type\" : \"service_account\" , \"project_id\" : \"abcde\" , \"private_key_id\" : \"\" , \"private_key\" : \"\" , \"client_email\" : \"abcde@appspot.gserviceaccount.com\" , \"client_id\" : \"XXXXXXXXXXXXX\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/abcde%40appspot.gserviceaccount.com\" } To add an access key in GCP: Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Locate your service account and in the Actions column, click the more button (three vertical dots). Click Manage keys Click ADD KEY \u2192 Create new key \u2192 set the radio button to JSON and click CREATE . Your browser begins the downloading of the key to your computer. Cloud Storage bucket To create a new Cloud Storage bucket: Log in to the Google Cloud console at the Cloud Storage product. Click CREATE BUCKET Read Google's Create storage buckets to learn how to set up your new bucket.","title":"Set up google bigquery"},{"location":"set-up-google-bigquery/#overview","text":"This page is a guide to setting up your Google Cloud Platform (GCP) account to use Google BigQuery as a destination within Matillion Data Loader.","title":"Overview"},{"location":"set-up-google-bigquery/#prerequisites","text":"For security purposes, you may wish to set up a new Google Cloud Platform (GCP) account and project for use with Matillion Data Loader. Make sure you have access to a GCP project. You need permissions in the GCP project to create Identity Access Management (IAM) services accounts. Make sure you have access to a Google Cloud Platform project within Google BigQuery. You need to connect to an IAM service account that has access to BigQuery, Cloud Storage, and your project. You need permissions to create a Cloud Storage bucket and a BigQuery dataset. You need administrative permission to a Matillion ETL for BigQuery instance that has billing enabled. It's recommended to have billing enabled while performing data loading processes even if you're using the free trial option.","title":"Prerequisites"},{"location":"set-up-google-bigquery/#enable-the-gcp-project-api","text":"If you have sufficient access to enable the API, follow the steps below to enable the API in your own Google Cloud project: Navigate to APIs & Services in the Google Cloud console. Click Library \u2192 Private . If you don't see an API listed, you don't have access to enable it. Click the API you want to enable. Use the search field to filter out unwanted objects. Enable the following APIs: BigQuery API Cloud Key Management Service (KMS) API (optional) Cloud Pub Sub API (optional) Cloud Resource Manager API Cloud Storage API Google Sheets API (if you want to use a Google Sheets data source). In the page that displays information about the API, click Enable .","title":"Enable the GCP project API"},{"location":"set-up-google-bigquery/#create-a-bigquery-dataset","text":"To create a dataset: Navigate to Google BigQuery . In the Explorer panel, select the more button (three vertical dots) on your project and click Create data set . Name the dataset in the Data set ID field. Set a dataset location . You can't change the dataset location once set. Select Enable table expiry if you wish to set a maximum age of a table in days. Click Create DATA SET . You can now choose this data set in the dropdown list within the project, and the Dataset ID can be used in Matillion ETL and Data Loader.","title":"Create a BigQuery dataset"},{"location":"set-up-google-bigquery/#creating-service-accounts","text":"Matillion Data Loader requires you to set up an existing GCP project with Google BigQuery and GCP authentication. Authentication is achieved via service accounts in Google\u2014an account within your GCP project used by virtual machines (VMs). A service account has assigned roles and permissions, and access keys. To create a service account: Make sure you have the BigQuery API enabled. Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Click + Create service account Enter a service account name to display in the Google Cloud console. The Google Cloud console generates a service account ID based on this name. Edit the ID if necessary. You can't change the ID later. Optional: Enter a description of the service account. Click Continue to progress to part 2, Grant this service account access to the project (optional) . Optional: Choose one or more IAM roles to grant to the service account on the project. Click Continue to progress to part 3, Grant users access to this service account (optional) . Optional: In the Service account users role field, add members that can impersonate the service account. Optional: In the Service account admins role field, add members that can manage the service account. Click Done to create the service account. Locate your new service account in Service accounts .","title":"Creating Service Accounts"},{"location":"set-up-google-bigquery/#add-roles-to-a-service-account","text":"Navigate to IAM & Admin: IAM in the Google Cloud console. Click the current project, and then click Open . In the Permission tab, locate your service account (Principal) and click the edit button (pencil). Add a role. Click + ADD ANOTHER ROLE if applicable. Click SAVE . Ensure these roles are present: BigQuery Admin Project Viewer Storage Admin","title":"Add roles to a service account"},{"location":"set-up-google-bigquery/#link-a-service-account-to-a-cloud-billing-account","text":"Your GCP project must have billing enabled. You must define a Cloud Billing account outside a project and then you must apply the Cloud Billing account to a project at your discretion. Navigate to Billing in the Google Cloud console. Select a Cloud Billing account for your chosen project in the list. To learn more about changing a Cloud Billing account for projects, read _Enable, disable, or change billing for a project _ . To learn how to create a Cloud Billing account, _Create, modify, or close your self-serve Cloud Billing account _ .","title":"Link a service account to a Cloud Billing account"},{"location":"set-up-google-bigquery/#access-keys","text":"Service accounts use keys to access services. You need a key to configure your Matillion ETL and Data Loader instances. The access key format is below: { \"type\" : \"service_account\" , \"project_id\" : \"abcde\" , \"private_key_id\" : \"\" , \"private_key\" : \"\" , \"client_email\" : \"abcde@appspot.gserviceaccount.com\" , \"client_id\" : \"XXXXXXXXXXXXX\" , \"auth_uri\" : \"https://accounts.google.com/o/oauth2/auth\" , \"token_uri\" : \"https://accounts.google.com/o/oauth2/token\" , \"auth_provider_x509_cert_url\" : \"https://www.googleapis.com/oauth2/v1/certs\" , \"client_x509_cert_url\" : \"https://www.googleapis.com/robot/v1/metadata/x509/abcde%40appspot.gserviceaccount.com\" } To add an access key in GCP: Navigate to IAM & Admin: Service Accounts in the Google Cloud console. Locate your service account and in the Actions column, click the more button (three vertical dots). Click Manage keys Click ADD KEY \u2192 Create new key \u2192 set the radio button to JSON and click CREATE . Your browser begins the downloading of the key to your computer.","title":"Access keys"},{"location":"set-up-google-bigquery/#cloud-storage-bucket","text":"To create a new Cloud Storage bucket: Log in to the Google Cloud console at the Cloud Storage product. Click CREATE BUCKET Read Google's Create storage buckets to learn how to set up your new bucket.","title":"Cloud Storage bucket"},{"location":"test/","text":"Hello, this is just a test page. Open Visual Studio Code. Create a file with the extension .swift Run the following code: var gemCounter = 0 while gemCounter < 1 { moveForward () if ! isBlockedRight { turnRight () } if isBlocked && isBlockedRight { turnLeft () } if isBlocked && isBlockedRight && isBlockedLeft { turnLeft () } if isBlocked && isBlockedRight && ! isBlockedLeft { turnLeft () } if isOnGem { collectGem () gemCounter += 1 } } Now, what was the name of the variable above? Snowflake Redshift BigQuery Property Description Name A human-readable name for the component. Warehouse Select a Snowflake warehouse. Property Description Name A human-readable name for the component. Schema Select a Redshift schema. Property Description Name A human-readable name for the component. Dataset Select a BigQuery dataset.","title":"Test"},{"location":"data-loader/","text":"Overview Welcome to the Matillion Data Loader documentation site where we hope you can find answers to your questions and problems as well as discovering new things that will enhance your ETL journey. If you are looking for ticketed support for a Matillion ETL issue, please visit the Matillion Support Portal {:target=\"_blank\"} or see our Getting Support {:target=\"_blank\"} information. If you are still using Matillion Data Loader v1.x, please refer to the Matillion Data Loader v1 documentation site {:target=\"_blank\"}. Category Description Data Loader Overview {:target=\"_blank\"} An introduction to the functions and concepts required to use Matillion Data Loader. Administration {:target=\"_blank\"} Extended information aimed at users administering Matillion Data Loader for themselves or their organization. Deploying CDC Agents {:target=\"_blank\"} Detailed information on CDC agent installation which is a requirement of CDC pipelines. Sources {:target=\"_blank\"} Configuration information for all Batch and CDC pipeline sources. Destinations {:target=\"_blank\"} How to connect to supported Destinations for Batch and CDC pipelines. Troubleshooting {:target=\"_blank\"} Information on common issues in using CDC pipelines.","title":"What is Matillion Data Loader?"},{"location":"data-loader/#overview","text":"Welcome to the Matillion Data Loader documentation site where we hope you can find answers to your questions and problems as well as discovering new things that will enhance your ETL journey. If you are looking for ticketed support for a Matillion ETL issue, please visit the Matillion Support Portal {:target=\"_blank\"} or see our Getting Support {:target=\"_blank\"} information. If you are still using Matillion Data Loader v1.x, please refer to the Matillion Data Loader v1 documentation site {:target=\"_blank\"}. Category Description Data Loader Overview {:target=\"_blank\"} An introduction to the functions and concepts required to use Matillion Data Loader. Administration {:target=\"_blank\"} Extended information aimed at users administering Matillion Data Loader for themselves or their organization. Deploying CDC Agents {:target=\"_blank\"} Detailed information on CDC agent installation which is a requirement of CDC pipelines. Sources {:target=\"_blank\"} Configuration information for all Batch and CDC pipeline sources. Destinations {:target=\"_blank\"} How to connect to supported Destinations for Batch and CDC pipelines. Troubleshooting {:target=\"_blank\"} Information on common issues in using CDC pipelines.","title":"Overview"},{"location":"data-loader/getting-support/","text":"Overview At Matillion, we love hearing from our customers. Communication with customers often gives us the opportunity to help users get more from their Matillion Data Loader experience, so please get in touch. We welcome questions, issues, or even general feedback, because these forms of communication help us to make a better product for all Matillion Data Loader users. Contacting Us Whether you are getting started with Matillion Data Loader, need technical advice, or are attempting to troubleshoot issues with the client and related services, we are eager to help. Sometimes the in-client or documented Help isn't enough to solve the challenges our customers face, and the best way for users to get further assistance is to either raise a ticket at our support portal or email our support team directly. Visit our support portal at support.matillion.com {:target=\"_blank\"}, from here you can: Create an account. Raise a Support Ticket - every page on our support site has the Raise A Support Ticket button. Post a public question. Ensure your \"Case Category\" is set to \"Matillion Data Loader\" Please adhere to the below table when submitting tickets to Matillion Support: Severity Code Description Severity 1 Critical production outage Severity 2 High impact loss of service Severity 3 Medium, partial impact to service delivery Severity 4 Low impact, minor issue, or information request","title":"Getting support"},{"location":"data-loader/getting-support/#overview","text":"At Matillion, we love hearing from our customers. Communication with customers often gives us the opportunity to help users get more from their Matillion Data Loader experience, so please get in touch. We welcome questions, issues, or even general feedback, because these forms of communication help us to make a better product for all Matillion Data Loader users.","title":"Overview"},{"location":"data-loader/getting-support/#contacting-us","text":"Whether you are getting started with Matillion Data Loader, need technical advice, or are attempting to troubleshoot issues with the client and related services, we are eager to help. Sometimes the in-client or documented Help isn't enough to solve the challenges our customers face, and the best way for users to get further assistance is to either raise a ticket at our support portal or email our support team directly. Visit our support portal at support.matillion.com {:target=\"_blank\"}, from here you can: Create an account. Raise a Support Ticket - every page on our support site has the Raise A Support Ticket button. Post a public question. Ensure your \"Case Category\" is set to \"Matillion Data Loader\" Please adhere to the below table when submitting tickets to Matillion Support: Severity Code Description Severity 1 Critical production outage Severity 2 High impact loss of service Severity 3 Medium, partial impact to service delivery Severity 4 Low impact, minor issue, or information request","title":"Contacting Us"},{"location":"data-loader/mongodb/","text":"Overview MongoDB is a source-available cross-platform document-oriented database program. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard. Create pipeline In Matillion Data Loader, click Add pipeline . Choose MongoDB from the grid of data sources. Choose Batch Loading . Connect to MongoDB Configure the MongoDB database connection settings, specifying the following: Property Description Connection URL The server IP or DNS address of your MongoDB server endpoint. Database The name of your MongoDB database. Username A valid login username for your MongoDB database server. Password A managed entry representing your MongoDB database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Connection String URI Format for reference of MongoDB connection options. Note The advanced connection settings AuthSchema:SCRAM-SHA-1 and AuthDatabase:admin will be defined by default. If your source is an Atlas cluster, you must also specify the following advanced connection settings: UseSSL: true SlaveOK: true You may also need to set ReplicaSet depending on your configuration: ReplicaSet=<your replica set name> . For example: cluster0-shard-00-01-test.mongodb.net:27017 , cluster0-shard-00-02-test.mongodb.net:27017 Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason. Flatten your objects Set the Flatten objects dropdown field to either Yes or No . The default is No . When Yes , Matillion Data Loader flattens and converts a nested data layer object into a new object with only one layer of key:value pairs. Choose tables Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward. Configure columns Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. Note Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table. Choose destination Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery. Set frequency Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"MongoDB"},{"location":"data-loader/mongodb/#overview","text":"MongoDB is a source-available cross-platform document-oriented database program. With Matillion Data Loader, you can replicate and load your source data into your target destination. Schema Drift Support : yes\u2014read Schema Drift to learn more. Return to any page of this wizard by clicking Previous . Click X in the upper-right of the UI and then click Yes, discard to close the pipeline creation wizard.","title":"Overview"},{"location":"data-loader/mongodb/#create-pipeline","text":"In Matillion Data Loader, click Add pipeline . Choose MongoDB from the grid of data sources. Choose Batch Loading .","title":"Create pipeline"},{"location":"data-loader/mongodb/#connect-to-mongodb","text":"Configure the MongoDB database connection settings, specifying the following: Property Description Connection URL The server IP or DNS address of your MongoDB server endpoint. Database The name of your MongoDB database. Username A valid login username for your MongoDB database server. Password A managed entry representing your MongoDB database login password. Choose an existing password from the dropdown menu or click Manage and then click Add new password to configure a new managed password entry. Give the password a label, which is what you can see in the password dropdown menu, and then input the value of the password. Read Manage Passwords to learn more. Advanced settings Additional JDBC parameters or connection settings. Click Advanced settings and then choose a parameter from the dropdown menu and enter a value for the parameter. Click Add parameter for each extra parameter you want to add. Read Connection String URI Format for reference of MongoDB connection options. Note The advanced connection settings AuthSchema:SCRAM-SHA-1 and AuthDatabase:admin will be defined by default. If your source is an Atlas cluster, you must also specify the following advanced connection settings: UseSSL: true SlaveOK: true You may also need to set ReplicaSet depending on your configuration: ReplicaSet=<your replica set name> . For example: cluster0-shard-00-01-test.mongodb.net:27017 , cluster0-shard-00-02-test.mongodb.net:27017 Click Test and Continue to test your settings and move forward. You can't continue if the test fails for any reason.","title":"Connect to MongoDB"},{"location":"data-loader/mongodb/#flatten-your-objects","text":"Set the Flatten objects dropdown field to either Yes or No . The default is No . When Yes , Matillion Data Loader flattens and converts a nested data layer object into a new object with only one layer of key:value pairs.","title":"Flatten your objects"},{"location":"data-loader/mongodb/#choose-tables","text":"Choose any tables you wish to include in the pipeline. Use the arrow buttons to move tables to the Tables to extract and load listbox and then reorder any tables with click-and-drag. Additionally, select multiple tables using the SHIFT key. Click Continue with X tables to move forward.","title":"Choose tables"},{"location":"data-loader/mongodb/#configure-columns","text":"Choose the columns from each table to include in the pipeline. By default, Matillion Data Loader selects all columns from a table. Click Configure on a table to open Configure table . This dialog lists columns in a table and the data type of each column. Additionally, you can set a primary key and assign an incremental column state to a column. Note Primary Key columns should represent a true PRIMARY KEY that uniquely identifies each record in a table. Composite keys work, but you must specify all columns that compose the key. Based on the primary key, this won't permit duplicate records. Jobs in Matillion ETL may fail or replicate data incorrectly if these rules aren't applied. Make sure an Incremental column is a true change data capture (CDC) column that can identify whether there has been a change for reach record in the table. This column should be a TIMESTAMP/DATE/DATETIME type or an INTEGER type representing a date key or UNIX timestamp. Click Add and remove columns to modify a table before a load. Use the arrow buttons to move columns out of the Columns to extract and load listbox. Order columns with click-and-drag. Select multiple columns using SHIFT . Click Done adding and removing to continue and then click Done . Click Continue once you have configured each table.","title":"Configure columns"},{"location":"data-loader/mongodb/#choose-destination","text":"Choose an existing destination or click Add a new destination . Select a destination from Snowflake, Amazon Redshift, or Google BigQuery.","title":"Choose destination"},{"location":"data-loader/mongodb/#set-frequency","text":"Property Description Pipeline name A descriptive label for your pipeline. This is how the pipeline appears on the pipeline dashboard and how Matillion Data Loader refers to the pipeline. Sync every The frequency at which the pipeline should sync. Day values include 1\u20147. Hour values include 1\u201423. Minute values include 5\u201459. The input is also the length of delay before the first sync. Currently, you can't specify a start time. Once you are happy with your pipeline configuration, click Create pipeline to complete the process and add the pipeline to your dashboard.","title":"Set frequency"},{"location":"etl/","text":"Some text.","title":"What is Matillion ETL?"},{"location":"etl/switch-project/","text":"Some text.","title":"Switch Project"}]}